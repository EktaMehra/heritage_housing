{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626e6ecc",
   "metadata": {},
   "source": [
    "_This project was developed independently as part of Code Institute’s Predictive Analytics Project. Any datasets or templates used are openly provided by the course or via public sources like Kaggle. All commentary and code logic are my own._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcfd32",
   "metadata": {},
   "source": [
    "# Notebook 02: Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4a569",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "- Identify and handle missing values.\n",
    "- Remove duplicates and inconsistencies.\n",
    "- Standardize formatting and data types.\n",
    "- Split the cleaned dataset.\n",
    "- Save the cleaned datasets.\n",
    "- Document the cleaning process.\n",
    "\n",
    "### Inputs\n",
    "- `/data/raw/house_prices_records.csv`  \n",
    "- `/data/raw/inherited_houses.csv`\n",
    "\n",
    "### Outputs\n",
    "- `/data/processed/df_cleaned.csv`  \n",
    "- Summary statistics for cleaned data  \n",
    "- Missing value report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f54a6",
   "metadata": {},
   "source": [
    "## Additional Observations\n",
    "- This notebook follows the Data Preparation stage of the CRISP-DM approach.\n",
    "- Decisions for cleaning (such as imputation techniques and handling outliers) are based on subject expertise and data properties.\n",
    "- The feature engineering notebook will cover outlier detection and management.\n",
    "- In later notebooks, the cleaned and divided datasets are prepared for activities like feature engineering, exploratory data analysis, and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561a4eb",
   "metadata": {},
   "source": [
    "## Change Working Directory\n",
    "- Since it is expected that you would keep the notebooks in a subfolder, you will need to switch the working directory when you run the notebook in the editor.\n",
    "- The working directory must be changed from its current folder to its parent folder.\n",
    "- We wish to change the current directory's parent to the new current directory.\n",
    "- Verify the updated current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c773c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Heritage Housing – Data Cleaning & Preparation Script\n",
    "\n",
    "Overview:\n",
    "This script handles data preparation for the Heritage Housing price prediction project. It performs the following steps:\n",
    "\n",
    "1. Loads raw house price and inherited property data\n",
    "2. Standardizes column names and identifies missing values\n",
    "3. Visualizes missing data percentages\n",
    "4. Applies numerical and categorical imputations using pipelines\n",
    "5. Conducts Pearson and Spearman correlation analysis\n",
    "6. Drops irrelevant or highly null features\n",
    "7. Converts and verifies datatypes\n",
    "8. Removes duplicates and finalizes a clean dataset\n",
    "9. Splits the data into training and test sets\n",
    "10. Saves all processed outputs in appropriate folders\n",
    "\n",
    "Outputs:\n",
    "- `data/processed/df_cleaned.csv`  \n",
    "- `data/processed/cleaned/house_prices_cleaned.csv`  \n",
    "- `data/processed/split/X_train.csv`, `y_train.csv`, `X_test.csv`, `y_test.csv`  \n",
    "- `outputs/visuals/missing_data_barchart.png`, `pearson_correlation_heatmap.png`\n",
    "\n",
    "Note:\n",
    "This step ensures the data is ready for downstream modeling and visualization pipelines.  \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# Smart Working Directory Setup\n",
    "project_root = '/workspaces/heritage_housing'\n",
    "if os.getcwd() != project_root:\n",
    "    try:\n",
    "        os.chdir(project_root)\n",
    "        print(f\"[INFO] Changed working directory to project root: {os.getcwd()}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"[ERROR] Project root '{project_root}' not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99677210",
   "metadata": {},
   "source": [
    "## Import Packages & Set Environment Variables\n",
    "- First, we need to import the numpy and pandas packages, and set the environment variables by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f6484",
   "metadata": {},
   "source": [
    "## Load Collected Data\n",
    "- Now that we have imported the required packages and environment variables set, we need to load the data that was previously downloaded (refer to the Data Collection notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data from the Data Collection stage\n",
    "df = pd.read_csv(\"data/raw/house_prices_records.csv\")\n",
    "house_prices_df = pd.read_csv(\"data/raw/house_prices_records.csv\")\n",
    "inherited_df = pd.read_csv(\"data/raw/inherited_houses.csv\")\n",
    "\n",
    "# Standardize column names immediately\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "house_prices_df.columns = house_prices_df.columns.str.replace(' ', '_')\n",
    "inherited_df.columns = inherited_df.columns.str.replace(' ', '_')\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing Values in House Prices Dataset (Absolute & Percentage):\")\n",
    "missing_df = pd.DataFrame({\n",
    "    \"Missing Values\": df.isnull().sum(),\n",
    "    \"Percentage\": (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "if missing_df[\"Missing Values\"].any():\n",
    "    display(missing_df[missing_df[\"Missing Values\"] > 0])\n",
    "else:\n",
    "    print(\"No missing values in the House Prices Dataset.\")\n",
    "\n",
    "print(\"\\nMissing Values in Inherited Houses Dataset (Absolute & Percentage):\")\n",
    "missing_inherited_df = pd.DataFrame({\n",
    "    \"Missing Values\": inherited_df.isnull().sum(),\n",
    "    \"Percentage\": (inherited_df.isnull().sum() / len(inherited_df) * 100).round(2)\n",
    "})\n",
    "if missing_inherited_df[\"Missing Values\"].any():\n",
    "    display(missing_inherited_df[missing_inherited_df[\"Missing Values\"] > 0])\n",
    "else:\n",
    "    print(\"No missing values in the Inherited Houses Dataset.\")\n",
    "\n",
    "# Other Checks\n",
    "print(f\"\\nHouse Prices Dataset Shape: {df.shape}\")\n",
    "print(f\"Inherited House Dataset Shape: {inherited_df.shape}\")\n",
    "\n",
    "print(\"\\nData Types in House Prices Dataset:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nData Types in Inherited Houses Dataset:\")\n",
    "print(inherited_df.dtypes)\n",
    "\n",
    "print(\"\\nPreview of House Prices Dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nPreview of Inherited Houses Dataset:\")\n",
    "display(inherited_df.head())\n",
    "\n",
    "print(\"\\nSummary Statistics for House Prices Dataset:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nSummary Statistics for Inherited Houses Dataset:\")\n",
    "display(inherited_df.describe())\n",
    "\n",
    "print(\"\\nCategorical Summary for House Prices Dataset:\")\n",
    "display(df.select_dtypes(include=['object']).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691581db",
   "metadata": {},
   "source": [
    "## Data Exploration – Missing Data\n",
    "### Missing Data Exploration\n",
    "\n",
    "**Identify Variables with Missing Data**\n",
    "- Generate a list of columns that have missing values and calculate their corresponding percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "missing_house_prices = house_prices_df.isnull().sum()\n",
    "missing_inherited = inherited_df.isnull().sum()\n",
    "\n",
    "# Display only columns with missing values\n",
    "print(\"\\nMissing values in House Prices Dataset:\")\n",
    "print(missing_house_prices[missing_house_prices > 0])\n",
    "\n",
    "print(\"\\nMissing values in Inherited Houses Dataset:\")\n",
    "print(missing_inherited[missing_inherited > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f3d53",
   "metadata": {},
   "source": [
    "**Assessing Missing Data Levels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d61572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_percentage(df):\n",
    "    percent_missing = df.isnull().sum() / len(df) * 100\n",
    "    return percent_missing[percent_missing > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nHouse Prices Missing Data Percentage:\")\n",
    "print(missing_percentage(house_prices_df))\n",
    "\n",
    "print(\"\\nInherited Houses Missing Data Percentage:\")\n",
    "print(missing_percentage(inherited_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909735e",
   "metadata": {},
   "source": [
    "**Classify Missing Data**\n",
    "\n",
    "Manual inspection and logical reasoning will help us categorize the nature of missingness\n",
    "\n",
    "Examples:\n",
    "- Garage-related columns may be missing if the house does not have a garage → Systematic.\n",
    "- Missing `Electrical` could be data entry error → Random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssessMissingData(df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with count and percentage of missing data in the given dataset.\n",
    "    \"\"\"\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0]\n",
    "    \n",
    "    if missing_data.empty:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if nothing's missing\n",
    "\n",
    "    percent_missing = (missing_data / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        \"MissingValues\": missing_data,\n",
    "        \"PercentageOfDataset\": percent_missing\n",
    "    })\n",
    "    \n",
    "    # Optional: Sort by highest percentage\n",
    "    missing_summary = missing_summary.sort_values(\"PercentageOfDataset\", ascending=False)\n",
    "\n",
    "    return missing_summary\n",
    "\n",
    "# Assess and plot missing data for House Prices Dataset\n",
    "df_missing_data_summary = AssessMissingData(house_prices_df)\n",
    "\n",
    "if not df_missing_data_summary.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        x=df_missing_data_summary.index,\n",
    "        y=df_missing_data_summary[\"PercentageOfDataset\"],\n",
    "        palette=\"viridis\",\n",
    "        legend=False,\n",
    "        hue=df_missing_data_summary.index,\n",
    "    )\n",
    "    plt.title(\"Percentage of Missing Data by Feature - House Prices Dataset\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Percentage of Missing Data\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/visuals/missing_data_barchart.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing data detected in the House Prices dataset.\")\n",
    "\n",
    "# Assess missing data for Inherited Houses Dataset\n",
    "print(\"\\nAssessing Missing Data in Inherited Houses Dataset:\")\n",
    "df_inherited_missing_data_summary = AssessMissingData(inherited_df)\n",
    "\n",
    "if not df_inherited_missing_data_summary.empty:\n",
    "    display(df_inherited_missing_data_summary)\n",
    "else:\n",
    "    print(\"No missing data detected in the Inherited Houses dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc2a36",
   "metadata": {},
   "source": [
    "## Missing Data Assessment Results\n",
    "\n",
    "**House Prices Dataset**\n",
    "\n",
    "**1. High Missingness:**\n",
    "\n",
    "- EnclosedPorch: ~90% missing — almost a ghost feature.\n",
    "- WoodDeckSF: ~89% missing — severely incomplete.\n",
    "\n",
    "**2. Moderate Missingness (~15-20%):**\n",
    "\n",
    "- LotFrontage\n",
    "- GarageFinish\n",
    "- BsmtFinType1\n",
    "\n",
    "**3. Low Missingness (<10%):**\n",
    "\n",
    "- BedroomAbvGr\n",
    "- 2ndFlrSF\n",
    "- GarageYrBlt\n",
    "- BsmtExposure\n",
    "- MasVnrArea\n",
    "\n",
    "**Insight:**\n",
    "- EnclosedPorch and `WoodDeckSF` have extremely high missingness and may need to be dropped unless they hold critical business value.\n",
    "- Other features can be imputed using median, mode, or model-based methods depending on their impact.\n",
    "\n",
    "## Inherited Houses Dataset\n",
    "- No missing data detected. Dataset is clean and ready.\n",
    "\n",
    "## Visual Analysis Summary\n",
    "- Most missingness appears feature-specific rather than across multiple columns.\n",
    "- No strong clustering patterns detected, suggesting missing at random (MAR) rather than systematic missingness.\n",
    "\n",
    "## Next Steps Recommendation\n",
    "- Drop `EnclosedPorch` and `WoodDeckSF` if they are not critical to analysis.\n",
    "- Impute other missing values using appropriate strategies based on feature type and importance.\n",
    "- Document missing data handling steps clearly for reproducibility and transparency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f60e26",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "**This section's objective is to:**\n",
    "\n",
    "- To find reliable predictors, assess the dataset's features' correlation with the target variable (SalePrice).\n",
    "- Evaluate feature multicollinearity to identify pairs of strongly linked features that could need extra care in subsequent stages.\n",
    "- To illustrate and summarize correlations and provide insights for feature selection and modeling, use heatmaps and numerical summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "house_prices_numeric = house_prices_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate Pearson correlation matrix\n",
    "pearson_corr = house_prices_numeric.corr(method='pearson')\n",
    "\n",
    "# Plot Pearson heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(pearson_corr, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title(\"Pearson Correlation Heatmap - House Prices Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/visuals/pearson_correlation_heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Spearman correlation matrix\n",
    "spearman_corr = house_prices_numeric.corr(method='spearman')\n",
    "\n",
    "# Plot Spearman heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(spearman_corr, annot=False, cmap='coolwarm')\n",
    "plt.title('Spearman Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca45140",
   "metadata": {},
   "source": [
    "## Multicollinearity Analysis (VIF)\n",
    "**Objectives of Multicollinearity Analysis (VIF):**\n",
    "- To detect if your features are lowkey sabotaging each other:\n",
    "If two or more predictors are heavily correlated, they basically tell the model the same story twice — making it confused, bloating the importance of useless features, and wrecking model stability.\n",
    "\n",
    "- To avoid inflated, unreliable model coefficients:\n",
    "When multicollinearity is high, your model starts throwing out wild coefficient values that make no sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import ArbitraryNumberImputer, CategoricalImputer, MeanMedianImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define imputers using original column casing\n",
    "numeric_imputer = MeanMedianImputer(imputation_method='median', variables=[\"LotFrontage\", \"MasVnrArea\"])\n",
    "categorical_imputer = CategoricalImputer(imputation_method=\"missing\", fill_value=\"None\", variables=[\"GarageFinish\", \"BsmtFinType1\"])\n",
    "garage_year_imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=[\"GarageYrBlt\"])\n",
    "specific_feature_imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=[\"2ndFlrSF\", \"BedroomAbvGr\"])\n",
    "\n",
    "# Pipeline\n",
    "imputation_pipeline = Pipeline([\n",
    "    (\"numeric_imputer\", numeric_imputer),\n",
    "    (\"categorical_imputer\", categorical_imputer),\n",
    "    (\"garage_year_imputer\", garage_year_imputer),\n",
    "    (\"specific_feature_imputer\", specific_feature_imputer)\n",
    "])\n",
    "\n",
    "# Apply to the cleaned dataset\n",
    "df_cleaned = imputation_pipeline.fit_transform(house_prices_df)\n",
    "\n",
    "# Check remaining missing values\n",
    "print(\"\\nRemaining missing values after imputation:\")\n",
    "print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0])\n",
    "\n",
    "# Sanity check\n",
    "print(\"\\nShape of cleaned dataframe:\", df_cleaned.shape)\n",
    "display(df_cleaned.head())\n",
    "\n",
    "# Save output\n",
    "df_cleaned.to_csv(\"data/processed/df_cleaned.csv\", index=False)\n",
    "print(\"\\ndf_cleaned.csv saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb3fc3",
   "metadata": {},
   "source": [
    "## VIF Scores Analysis\n",
    "**Highest VIFs:**\n",
    "- GarageYrBlt (4.15)\n",
    "- TotalBsmtSF (3.90)\n",
    "- YearBuilt (3.78)\n",
    "- 1stFlrSF (3.73)\n",
    "\n",
    "**Mid VIFs:**\n",
    "- OverallQual, GrLivArea, GarageArea, YearRemodAdd\n",
    "\n",
    "**Low VIFs:**\n",
    "- BsmtFinSF1, MasVnrArea\n",
    "\n",
    "### Conclusion\n",
    "The Variance Inflation Factor (VIF) analysis indicates that all selected features have VIF scores below 5, suggesting no severe multicollinearity is present. Therefore, all features can be retained for modeling without immediate concern for redundancy or instability.\n",
    "\n",
    "**Action Required?**\n",
    "- No immediate drop or transformation needed based on VIF.\n",
    "- Keep an eye on the \"GarageYrBlt\", \"TotalBsmtSF\" area when feature engineering — minor tweaking might help model efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036194a",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data: \n",
    "\n",
    "### Drop Variables:\n",
    "**Purpose:**\n",
    "\n",
    "Some variables are dropped because they either:\n",
    "- Do not provide meaningful predictive power (e.g., IDs, irrelevant details),\n",
    "- Introduce redundancy (highly correlated with other variables),\n",
    "- Leak target information (variables that reveal the outcome directly),\n",
    "- Increase noise and model complexity without adding value.\n",
    "\n",
    "**By removing such variables, we aim to:**\n",
    "- Improve model interpretability,\n",
    "- Reduce the risk of overfitting,\n",
    "- Enhance model training efficiency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c32d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview all the columns\n",
    "print(\"Columns in house_prices_df:\")\n",
    "print(house_prices_df.columns)\n",
    "\n",
    "# Drop columns with high missing values and low correlation\n",
    "columns_to_drop = ['EnclosedPorch', 'WoodDeckSF']\n",
    "house_prices_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(\"Dropped features due to excessive missing data or low correlation:\")\n",
    "print(columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48a22c",
   "metadata": {},
   "source": [
    "### Expected Outcomes\n",
    "- The features removed contribute little predictive value and their absence will not degrade model performance.\n",
    "- The dataset is now cleaner, leaner, and easier to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6531f",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n",
    "\n",
    "### Purpose:\n",
    "**To address missing data effectively and maintain the integrity of the dataset, the following imputation strategies will be applied:**\n",
    "\n",
    "- LotFrontage (17.74% missing): Impute using the median value to minimize the influence of extreme outliers while preserving the central tendency of the data.\n",
    "- GarageFinish (11.10% missing): Impute using the mode (\"None\") because missing values typically correspond to houses without garages, aligning logically with the property attributes.\n",
    "- GarageYrBlt (5.55% missing): Replace missing values with 0 to represent properties that have no garage, ensuring consistency with related features.\n",
    "- BsmtFinType1 (7.81% missing): Impute with \"None\" to reflect the absence of a finished basement in properties where this feature is not applicable.\n",
    "- Other Numeric Variables: Apply median imputation across numeric features to maintain robustness against skewed distributions and to avoid bias introduced by mean imputation.\n",
    "\n",
    "A dedicated preprocessing pipeline will be implemented to automate these imputations, ensuring that missing data handling is systematic, reproducible, and scalable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b57242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import ArbitraryNumberImputer, CategoricalImputer, MeanMedianImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define pipelines for numeric and categorical variables\n",
    "numeric_imputer = MeanMedianImputer(imputation_method='median', variables=[\"LotFrontage\", \"MasVnrArea\"])\n",
    "categorical_imputer = CategoricalImputer(imputation_method=\"missing\", fill_value=\"None\", variables=[\"GarageFinish\", \"BsmtFinType1\"])\n",
    "garage_year_imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=[\"GarageYrBlt\"])\n",
    "specific_feature_imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=[\"2ndFlrSF\", \"BedroomAbvGr\"])\n",
    "\n",
    "# Combine into a pipeline\n",
    "imputation_pipeline = Pipeline([\n",
    "    (\"numeric_imputer\", numeric_imputer),\n",
    "    (\"categorical_imputer\", categorical_imputer),\n",
    "    (\"garage_year_imputer\", garage_year_imputer),\n",
    "    (\"specific_feature_imputer\", specific_feature_imputer)\n",
    "])\n",
    "\n",
    "# Fit and transform on house_prices_df\n",
    "df_cleaned = imputation_pipeline.fit_transform(house_prices_df)\n",
    "\n",
    "# Check to confirm no missing values remain\n",
    "print(\"\\nRemaining missing values after imputation:\")\n",
    "print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0])\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"\\nShape of cleaned dataframe:\", df_cleaned.shape)\n",
    "display(df_cleaned.head())\n",
    "\n",
    "# Save cleaned data\n",
    "df_cleaned.to_csv(\"data/processed/df_cleaned.csv\", index=False)\n",
    "print(\"\\n df_cleaned.csv saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda0907",
   "metadata": {},
   "source": [
    "### Expected Results\n",
    "- All previously identified missing values are now filled.\n",
    "- Median is robust against outliers for numeric data.\n",
    "- Mode ensures the most frequent category is used for missing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995339bf",
   "metadata": {},
   "source": [
    "## Standardised Formatting\n",
    "\n",
    "We must make sure that every feature is in the right format and follows a defined structure in order to get the dataset ready for modeling. This stage entails:\n",
    "\n",
    "- Transforming numerical columns into the appropriate data types, such as floating or integers.\n",
    "- To guarantee consistency, categorical variable formats should be standardized.\n",
    "- Datetime formats are aligned where appropriate.\n",
    "- Standardizing data guarantees algorithm compatibility and reduces errors during machine learning procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise column names: replace spaces with underscores\n",
    "house_prices_df.columns = house_prices_df.columns.str.replace(' ', '_')\n",
    "\n",
    "\n",
    "# Convert data types (if any need converting)\n",
    "numeric_cols = house_prices_df.select_dtypes(include=['object']).columns\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        house_prices_df[col] = pd.to_numeric(house_prices_df[col])\n",
    "    except ValueError:\n",
    "        pass  # skip non-convertible columns\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = house_prices_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Display updated column names and data types\n",
    "print(\"\\nStandardised column names and data types:\")\n",
    "print(house_prices_df.dtypes)\n",
    "\n",
    "# Confirm Data Types and Consistency\n",
    "print(\"\\n[INFO] After Conversion - Data Types for Numeric Columns:\")\n",
    "print(df_cleaned[numeric_cols].dtypes)\n",
    "\n",
    "print(\"\\n[INFO] Sample Data for Numeric Columns:\")\n",
    "display(df_cleaned[numeric_cols].head())\n",
    "\n",
    "print(\"\\n[INFO] Sample Data for Categorical Columns:\")\n",
    "display(df_cleaned[categorical_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46eb3d0",
   "metadata": {},
   "source": [
    "### Expected Results\n",
    "- All column names are now consistent, lowercase, and snake_case.\n",
    "- Numeric data types are standardized for future modeling steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b756a",
   "metadata": {},
   "source": [
    "## Remove Duplicates and Inconsistencies\n",
    "\n",
    "**Purpose:**\n",
    "- Make sure there are no duplicate rows in the dataset, as this can cause redundancy and skew analysis.\n",
    "- Verify that numerical and category values are within the anticipated ranges and correct any discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "initial_shape = df_cleaned.shape\n",
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "final_shape = df_cleaned.shape\n",
    "\n",
    "print(f\"\\n[INFO] Duplicates Removed: {initial_shape[0] - final_shape[0]}\")\n",
    "print(f\"[INFO] New Dataset Shape: {final_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a09b8f",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "\n",
    "**Purpose:**\n",
    "- The model's ability to generalize to new data is ensured by separating the data into training and testing sets.\n",
    "- While the testing set assesses the model's performance on fresh, untested data, the training set is used to train the model.\n",
    "\n",
    "**Our Objective:**\n",
    "- Preserve the overall data distribution while keeping the training and testing sets balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the Data into Features (X) and Target (y)\n",
    "X = df_cleaned.drop('SalePrice', axis=1)\n",
    "y = df_cleaned['SalePrice']\n",
    "\n",
    "# Use a standard splitting ratio, such as 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Check the split\n",
    "print(f\"\\n[INFO] Training Set Shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"[INFO] Test Set Shape: {X_test.shape}, {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323aa4e",
   "metadata": {},
   "source": [
    "## Save Processed Datasets\n",
    "\n",
    "The cleaned dataset and the training and testing datasets will be saved in the relevant output directories in this section. For ease of access in later phases of this project, the structure guarantees that all processed data is centralized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c52525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define directories for saving cleaned and split datasets\n",
    "base_dir = \"data/processed\"\n",
    "cleaned_data_dir = os.path.join(base_dir, \"cleaned\")\n",
    "split_data_dir = os.path.join(base_dir, \"split\")\n",
    "\n",
    "# Create directories if they don't already exist\n",
    "os.makedirs(cleaned_data_dir, exist_ok=True)\n",
    "os.makedirs(split_data_dir, exist_ok=True)\n",
    "\n",
    "# Saving cleaned data to the specified directory\n",
    "cleaned_data_file = os.path.join(cleaned_data_dir, \"house_prices_cleaned.csv\")\n",
    "df_cleaned.to_csv(cleaned_data_file, index=False)\n",
    "print(f\"Cleaned dataset saved at: {cleaned_data_file}\")\n",
    "\n",
    "# Saving split datasets (train and test) to the specified directory\n",
    "train_data_paths = {\n",
    "    'X_train': os.path.join(split_data_dir, \"X_train.csv\"),\n",
    "    'y_train': os.path.join(split_data_dir, \"y_train.csv\"),\n",
    "    'X_test': os.path.join(split_data_dir, \"X_test.csv\"),\n",
    "    'y_test': os.path.join(split_data_dir, \"y_test.csv\")\n",
    "}\n",
    "\n",
    "# Save the datasets to the respective paths\n",
    "X_train.to_csv(train_data_paths['X_train'], index=False)\n",
    "y_train.to_csv(train_data_paths['y_train'], index=False)\n",
    "X_test.to_csv(train_data_paths['X_test'], index=False)\n",
    "y_test.to_csv(train_data_paths['y_test'], index=False)\n",
    "\n",
    "print(f\"Training and testing datasets have been saved to: {split_data_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b1c99",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "**Summary:**\n",
    "- Explored, cleaned, and preprocessed the house prices dataset.\n",
    "- Strategically imputed missing values.\n",
    "- Removed duplicate entries and ensured data consistency.\n",
    "- Split the dataset into training and testing subsets.\n",
    "- Saved processed datasets for future model development.\n",
    "\n",
    "**Next Steps:**\n",
    "- To perform Feature Engineering and Transformation.\n",
    "- Train baseline regression models.\n",
    "- Conduct hyperparameter tuning for optimization.\n",
    "- Evaluate model performance and identify key drivers that impact house prices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
