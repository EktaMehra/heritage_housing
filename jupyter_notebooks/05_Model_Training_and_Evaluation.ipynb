{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7920e7a3",
   "metadata": {},
   "source": [
    "_This project was developed independently as part of Code Institute’s Predictive Analytics Project. Any datasets or templates used are openly provided by the course or via public sources like Kaggle. All commentary and code logic are my own._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2cbde",
   "metadata": {},
   "source": [
    "# 05 Model Training And Evaluation\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook aims to train, evaluate, and compare multiple machine learning models to predict house sale prices using features engineered in earlier stages.\n",
    "Given our decision to retain outliers, tree-based models (especially ensemble methods) are expected to perform better.\n",
    "\n",
    "### Inputs  \n",
    "- `/data/processed/final/X_train.csv`  \n",
    "- `/data/processed/final/X_test.csv`  \n",
    "- `/data/processed/final/y_train.csv`  \n",
    "- `/data/processed/final/y_test.csv`\n",
    "\n",
    "### Outputs\n",
    "- Best model files: `/outputs/models/`  \n",
    "- Evaluation metrics: `/outputs/metrics/`  \n",
    "- Feature importances: `/outputs/ft_importance/`  \n",
    "- SHAP values: `/outputs/shap_values/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ff73e",
   "metadata": {},
   "source": [
    "## Change Working Directory\n",
    "- Since it is expected that you would keep the notebooks in a subfolder, you will need to switch the working directory when you run the notebook in the editor.\n",
    "- The working directory must be changed from its current folder to its parent folder.\n",
    "- We wish to change the current directory's parent to the new current directory.\n",
    "- Verify the updated current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f78cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart Working Directory Setup\n",
    "import os\n",
    "project_root = '/workspaces/heritage_housing'\n",
    "if os.getcwd() != project_root:\n",
    "    try:\n",
    "        os.chdir(project_root)\n",
    "        print(f\"[INFO] Changed working directory to project root: {os.getcwd()}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"[ERROR] Project root '{project_root}' not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848417b7",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "- Load libraries\n",
    "- Load cleaned data from /data/processed/df_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load processed data (engineered and encoded)\n",
    "X_train = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "X_test = pd.read_csv(\"data/processed/final/X_test.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/final/y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"data/processed/final/y_test.csv\").squeeze()\n",
    "\n",
    "# Reconstruct df-like object for compatibility with rest of the notebook\n",
    "df = pd.concat([X_train, y_train.rename(\"LogSalePrice\")], axis=1)\n",
    "\n",
    "print(\"[INFO] Final engineered data loaded successfully.\")\n",
    "print(f\"[INFO] Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\\n\")\n",
    "\n",
    "# Preview\n",
    "print(\"[INFO] Sample of training dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "\n",
    "if not missing.empty:\n",
    "    print(\"\\n[WARNING] Missing values detected:\")\n",
    "    display(missing)\n",
    "else:\n",
    "    print(\"[INFO] No missing values found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38576a72",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Split the cleaned dataset into training and testing subsets to evaluate model generalizability and avoid overfitting.\n",
    "\n",
    "**Why Train-Test Split?**\n",
    "\n",
    "- Prevents data leakage by ensuring evaluation is done on unseen data.\n",
    "- Provides a realistic estimate of model performance before deployment.\n",
    "- Essential for tracking overfitting or underfitting during experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf47eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y_train.head())\n",
    "\n",
    "# Confirm shape of the splits\n",
    "print(f\"[INFO] X_train shape: {X_train.shape}\")\n",
    "print(f\"[INFO] X_test shape:  {X_test.shape}\")\n",
    "print(f\"[INFO] y_train shape: {y_train.shape}\")\n",
    "print(f\"[INFO] y_test shape:  {y_test.shape}\")\n",
    "\n",
    "# Preview training data\n",
    "print(\"\\n[INFO] Preview of X_train:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\n[INFO] Preview of y_train:\")\n",
    "display(y_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b0212",
   "metadata": {},
   "source": [
    "### Baseline Model Evaluation\n",
    "\n",
    "Before diving into complex tuning or ensembling, it's crucial to evaluate some baseline regressors. This gives us a performance benchmark that future models must beat. It also helps identify underfitting or misalignment between features and the target variable.\n",
    "\n",
    "**We’ll begin with:**\n",
    "\n",
    "- Linear Regression – to test linearity assumptions\n",
    "- Decision Tree Regressor – to capture non-linear patterns\n",
    "- Random Forest Regressor – as a robust tree-based ensemble baseline\n",
    "\n",
    "We’ll use R2, MAE, and RMSE for evaluation.\n",
    "\n",
    "**Encode Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1135c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load feature-engineered training and testing data\n",
    "X_train = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "X_test = pd.read_csv(\"data/processed/final/X_test.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/final/y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"data/processed/final/y_test.csv\").squeeze()\n",
    "\n",
    "print(\"Feature-engineered data loaded successfully.\\n\")\n",
    "\n",
    "# Dataset structure checks\n",
    "print(\"Training Features Dataset Info:\")\n",
    "print(X_train.info())\n",
    "\n",
    "print(\"\\nTraining Target Stats:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "print(\"\\nTesting Features Dataset Info:\")\n",
    "print(X_test.info())\n",
    "\n",
    "print(\"\\nTesting Target Stats:\")\n",
    "print(y_test.describe())\n",
    "\n",
    "print(\"\\nPreview of X_train:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nPreview of X_test:\")\n",
    "display(X_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48605a50",
   "metadata": {},
   "source": [
    "**Define Evaluation Function**\n",
    "\n",
    "- To evaluate and compare multiple models quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Baseline: predict the mean of y_train\n",
    "y_baseline_train = [y_train.mean()] * len(y_train)\n",
    "y_baseline_test = [y_train.mean()] * len(y_test)\n",
    "\n",
    "# Evaluate metrics\n",
    "baseline_metrics = {\n",
    "    \"Dataset\": [\"Train\", \"Test\"],\n",
    "    \"R2\": [\n",
    "        r2_score(y_train, y_baseline_train),\n",
    "        r2_score(y_test, y_baseline_test)\n",
    "    ],\n",
    "    \"MAE\": [\n",
    "        mean_absolute_error(y_train, y_baseline_train),\n",
    "        mean_absolute_error(y_test, y_baseline_test)\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        np.sqrt(mean_squared_error(y_train, y_baseline_train)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_baseline_test))\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "baseline_df = pd.DataFrame(baseline_metrics)\n",
    "print(\"Baseline Model Metrics:\")\n",
    "display(baseline_df)\n",
    "\n",
    "# Visualize: predictions vs actual (test set)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_baseline_test, alpha=0.6, label=\"Baseline Predictions\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label=\"Perfect Fit\")\n",
    "plt.title(\"Baseline Model: Predictions vs Actual (Test Set)\")\n",
    "plt.xlabel(\"Actual LogSalePrice\")\n",
    "plt.ylabel(\"Predicted LogSalePrice\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c73131",
   "metadata": {},
   "source": [
    "### Baseline Model Performance\n",
    "\n",
    "#### Overview\n",
    "\n",
    "The baseline model predicts the mean log-transformed sale price (LogSalePrice) for all observations. It doesn’t use any features — just averages — and serves as a reference point. Every meaningful model going forward must beat this benchmark.\n",
    "\n",
    "**Results Summary**\n",
    "\n",
    "| Dataset   | R2 Score |  MAE     | MSE \n",
    "\n",
    "| Training  | 0.00000  | 0.303367 | 0.152442\n",
    "\n",
    "| Testing   | -0.00584 | 0.337137 | 0.187703\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "- R2: As expected, the training R2 is 0.00 (predicts the mean), and testing R2 is negative — the model performs worse than just guessing the average.\n",
    "- MAE: Around 0.30–0.34, which is significant. There’s room for improvement.\n",
    "- MSE: Reinforces the poor performance — it penalizes larger errors more heavily.\n",
    "\n",
    "#### Visualization Insights\n",
    "\n",
    "**The scatterplot clearly shows:**\n",
    "\n",
    "- Predictions are a flat horizontal line (the mean LogSalePrice).\n",
    "- True values (Y-axis) vary widely, exposing the model’s inability to generalize.\n",
    "- The dotted diagonal (perfect predictions) lies far from the flat baseline.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Ignores feature patterns.\n",
    "- Offers no learning capability.\n",
    "- Not meant for prediction — only for benchmarking.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Train advanced models:**\n",
    "\n",
    "- Linear, Ridge, Lasso\n",
    "- Decision Tree, Random Forest, Gradient Boosting, SVR\n",
    "- Track the same metrics (R2, MAE, RMSE) for fair comparison.\n",
    "- Use cross-validation and hyperparameter tuning to optimize.\n",
    "- Visualize residuals and analyze feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163913e",
   "metadata": {},
   "source": [
    "## Model Training & Hyperparameter Tuning\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Train multiple regression models to predict LogSalePrice and identify the best performer based on validation metrics. We'll use Grid Search for tuning where applicable, and visualize the results for comparison.\n",
    "\n",
    "**Models Considered:**\n",
    "\n",
    "We’ll train the following models to evaluate both linear and non-linear performance:\n",
    "\n",
    "- `Linear Regression`: Simple baseline for linear trends\n",
    "- `Ridge Regression`: Penalizes large coefficients (L2)\n",
    "- `Lasso Regression`: Useful for feature selection (L1)\n",
    "- `Decision Tree`: Handles non-linear splits, interpretable\n",
    "- `Random Forest`: Ensemble of trees to reduce variance\n",
    "- `Gradient Boosting Regressor`: Builds trees sequentially, optimizes performance\n",
    "- `Support Vector Regressor`: Captures complex relationships, especially in high dimensions\n",
    "- `K-Nearest Neighbors`: Instance-based learner for pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e88b3",
   "metadata": {},
   "source": [
    "### Train a Variety of Models\n",
    "\n",
    "This section will include:\n",
    "\n",
    "- Use the training dataset to train many machine learning models.\n",
    "- Use important metrics like R1, MAE, and MSE to compare how well they performed on the testing dataset.\n",
    "- Determine which models perform best and merit further refinement in the following steps by visualizing and analyzing the data.\n",
    "- Finding top-performing models to forecast home values based on important characteristics and associated hyperparameter setups is the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd15566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import Required Libraries ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# === Reload Data with Encoding ===\n",
    "X_train_raw = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/final/X_test.csv\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X_train_raw.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X_train = pd.get_dummies(X_train_raw, columns=categorical_cols, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test_raw, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Load target\n",
    "y_train = pd.read_csv(\"data/processed/final/y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"data/processed/final/y_test.csv\").squeeze()\n",
    "\n",
    "# === Define Evaluation Function ===\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test, plot=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Trains a model and evaluates its performance on training and test data.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): Name of the model.\n",
    "        model (sklearn estimator): The machine learning model to evaluate.\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        y_test (pd.Series): Test target.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with model performance metrics (R2, MAE, RMSE).\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    def get_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            \"R2\": r2_score(y_true, y_pred),\n",
    "            \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        }\n",
    "\n",
    "    train_metrics = get_metrics(y_train, y_train_pred)\n",
    "    test_metrics = get_metrics(y_test, y_test_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{name} Performance:\")\n",
    "        print(\"Train:\")\n",
    "        for k, v in train_metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "        print(\"Test:\")\n",
    "        for k, v in test_metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    if plot:\n",
    "        residuals = y_test - y_test_pred\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title(f\"{name} Residual Distribution\")\n",
    "        plt.xlabel(\"Residuals\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.scatterplot(x=y_test, y=y_test_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.title(f\"{name} Predicted vs Actual\")\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Train R2\": train_metrics[\"R2\"],\n",
    "        \"Train MAE\": train_metrics[\"MAE\"],\n",
    "        \"Train RMSE\": train_metrics[\"RMSE\"],\n",
    "        \"Test R2\": test_metrics[\"R2\"],\n",
    "        \"Test MAE\": test_metrics[\"MAE\"],\n",
    "        \"Test RMSE\": test_metrics[\"RMSE\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099aae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define and Evaluate Models ===\n",
    "models = [\n",
    "    (\"Linear Regression\", LinearRegression()),\n",
    "    (\"Decision Tree\", DecisionTreeRegressor(random_state=42)),\n",
    "    (\"Random Forest\", RandomForestRegressor(random_state=42)),\n",
    "    (\"Support Vector Regressor\", SVR()),\n",
    "    (\"K-Nearest Neighbors\", KNeighborsRegressor())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, model in models:\n",
    "    result = evaluate_model(name, model, X_train, X_test, y_train, y_test)\n",
    "    results.append(result)\n",
    "\n",
    "# === Display Results Summary ===\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Evaluation Summary:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12beca3",
   "metadata": {},
   "source": [
    "#### Filter Out Extreme Models\n",
    "\n",
    "If you're running multiple baselines or test runs, you may want to remove models with clearly poor R2 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.columns)\n",
    "\n",
    "# Filter out extreme underperformers\n",
    "filtered_results_df = results_df[results_df[\"Test R2\"] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f53b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visual Comparison of Model Performance (Train vs Test)\n",
    "\n",
    "This section plots R2, MAE, and RMSE for all models,\n",
    "split by training and testing datasets, using bar charts.\n",
    "Helps identify overfitting/underfitting and supports model selection.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"outputs/visuals\", exist_ok=True)\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "melted_df = results_df.melt(\n",
    "    id_vars=[\"Model\"],\n",
    "    value_vars=[\"Train R2\", \"Test R2\", \"Train MAE\", \"Test MAE\", \"Train RMSE\", \"Test RMSE\"],\n",
    "    var_name=\"MetricType\",\n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "# Split 'MetricType' into two columns: 'Metric' and 'Set'\n",
    "melted_df[[\"Set\", \"Metric\"]] = melted_df[\"MetricType\"].str.split(\" \", expand=True)\n",
    "melted_df.drop(\"MetricType\", axis=1, inplace=True)\n",
    "\n",
    "# Set aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plotting loop for each metric\n",
    "for metric in [\"R2\", \"MAE\", \"RMSE\"]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metric_df = melted_df[melted_df[\"Metric\"] == metric]\n",
    "    sns.barplot(data=metric_df, x=\"Score\", y=\"Model\", hue=\"Set\", palette=\"Set2\")\n",
    "    plt.title(f\"{metric} Score Comparison: Training vs Testing\")\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.legend(title=\"Dataset\", loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save each plot\n",
    "    filepath = f\"outputs/visuals/{metric.lower()}_score_comparison.png\"\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc9262",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "**Performance Overview:** \n",
    "- The testing dataset's highest R2 score (0.81) was attained by the Random Forest model, suggesting that it well captures the target variable.\n",
    "- The Decision Tree model demonstrated signs of overfitting with a lower testing R2 (0.74), but it did remarkably well on the training dataset (R2: 0.999).\n",
    "- With low R2 values for testing, both Linear Regression and Support Vector Regressor fared poorly, suggesting poor generalization.\n",
    "\n",
    "**Suitability for Additional adjustment:** \n",
    "- Because of their comparatively good testing results, models like Random Forest and Decision Tree make excellent candidates for hyperparameter adjustment.\n",
    "- To increase performance, models like SVR and K-Nearest Neighbours might need to be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bdf4f",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "#### Objectives:\n",
    "To enhance the performance of the best-performing models on the testing dataset, this part attempts to adjust their hyperparameters.\n",
    "\n",
    "**Models Chosen for Adjustment**\n",
    "- Random Forest\n",
    "- Decision Tree\n",
    "- Gradient Boosting\n",
    "- Ridge\n",
    "- SVR (Support Vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3211779",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27132a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random Forest Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "This block performs exhaustive search across specified hyperparameter combinations\n",
    "for a Random Forest Regressor to optimize model performance using 5-fold cross-validation.\n",
    "Evaluated using R2 score.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Define hyperparameter grid ---\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [100, 200],         # Number of trees in the forest\n",
    "    \"max_depth\": [10, 20, None],        # Tree depth control\n",
    "    \"min_samples_split\": [2, 5],        # Minimum samples required to split an internal node\n",
    "    \"min_samples_leaf\": [1, 2],         # Minimum samples required at each leaf node\n",
    "}\n",
    "\n",
    "# --- Initialize GridSearchCV ---\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Execute grid search ---\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Output best parameters and cross-validated R2 score ---\n",
    "print(\"Best Parameters for Random Forest:\", rf_grid_search.best_params_)\n",
    "print(\"Best R2-Score for Random Forest:\", rf_grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e289a",
   "metadata": {},
   "source": [
    "**Evaluate Best Random Forest Model on Test Data**\n",
    "\n",
    "- This will use the best parameters found during GridSearchCV to make predictions on the test set and compute the R2, MAE, and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Use the best estimator from GridSearchCV\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "rf_test_preds = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "r2 = r2_score(y_test, rf_test_preds)\n",
    "mae = mean_absolute_error(y_test, rf_test_preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, rf_test_preds))\n",
    "\n",
    "print(\"Tuned Random Forest Test Set Performance:\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"MAE:      {mae:.4f}\")\n",
    "print(f\"RMSE:     {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecbfe8",
   "metadata": {},
   "source": [
    "**Visualize Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166fd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "importances = best_rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 15 important features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df.head(15), x=\"Importance\", hue=\"Feature\", y=\"Feature\", palette=\"viridis\", legend=False)\n",
    "plt.title(\"Top 15 Feature Importances - Random Forest Regressor\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"outputs/visuals/rf_feature_importance_top15.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee28ca5",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# === Reload Data with Encoding ===\n",
    "X_train_raw = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/final/X_test.csv\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X_train_raw.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# One-hot encode\n",
    "X_train = pd.get_dummies(X_train_raw, columns=categorical_cols, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test_raw, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align columns to ensure matching features\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Load target\n",
    "y_train = pd.read_csv(\"data/processed/final/y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"data/processed/final/y_test.csv\").squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee46afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Decision Tree\n",
    "dt_param_grid = {\n",
    "    \"max_depth\": [3, 5, 10, 15, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "dt_grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=dt_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and parameters\n",
    "best_dt_model = dt_grid_search.best_estimator_\n",
    "best_dt_params = dt_grid_search.best_params_\n",
    "best_dt_score = dt_grid_search.best_score_\n",
    "\n",
    "print(\"Best Decision Tree Parameters:\", best_dt_params)\n",
    "print(\"Best CV R2 Score:\", round(best_dt_score, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696e5ef",
   "metadata": {},
   "source": [
    "**Evaluate Tuned Decision Tree Performance on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_dt = best_dt_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "dt_r2 = r2_score(y_test, y_pred_dt)\n",
    "dt_mae = mean_absolute_error(y_test, y_pred_dt)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "\n",
    "# Display performance\n",
    "print(\"Tuned Decision Tree Performance on Test Data:\")\n",
    "print(f\"R2 Score:   {dt_r2:.4f}\")\n",
    "print(f\"MAE:        {dt_mae:.4f}\")\n",
    "print(f\"RMSE:       {dt_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b7c20",
   "metadata": {},
   "source": [
    "**Visualize Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33501f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "y_pred_dt = best_dt_model.predict(X_test)\n",
    "dt_r2 = r2_score(y_test, y_pred_dt)\n",
    "dt_mae = mean_absolute_error(y_test, y_pred_dt)\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "\n",
    "print(\"\\nTuned Decision Tree Performance on Test Data:\")\n",
    "print(f\"R2 Score: {dt_r2:.4f}\")\n",
    "print(f\"MAE:      {dt_mae:.4f}\")\n",
    "print(f\"RMSE:     {dt_rmse:.4f}\")\n",
    "\n",
    "# Visualize top 15 features\n",
    "dt_feature_importances = best_dt_model.feature_importances_\n",
    "dt_features = X_train.columns\n",
    "dt_importance_df = pd.DataFrame({\n",
    "    \"Feature\": dt_features,\n",
    "    \"Importance\": dt_feature_importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=dt_importance_df.head(15), x=\"Importance\", y=\"Feature\", palette=\"viridis\", hue=\"Feature\", legend=False)\n",
    "plt.title(\"Top 15 Feature Importances - Decision Tree Regressor\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"outputs/visuals\", exist_ok=True)\n",
    "plt.savefig(\"outputs/visuals/dt_feature_importance_top15.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871a2bc",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea66329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Boosting Regressor - Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "This block searches for the best combination of hyperparameters for the Gradient Boosting model \n",
    "to maximize R2 performance via 5-fold cross-validation.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Define hyperparameter grid ---\n",
    "gbr_param_grid = {\n",
    "    \"n_estimators\": [100, 200],          # Number of boosting stages\n",
    "    \"max_depth\": [3, 4, 5],              # Max depth of each individual tree\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],  # Step size shrinkage\n",
    "    \"min_samples_split\": [2, 5],         # Min samples required to split a node\n",
    "    \"min_samples_leaf\": [1, 2]           # Min samples at a leaf node\n",
    "}\n",
    "\n",
    "# --- Initialize GridSearchCV ---\n",
    "gbr_grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(random_state=42),\n",
    "    param_grid=gbr_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Execute grid search ---\n",
    "gbr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Output best parameters and cross-validation R2 score ---\n",
    "print(\"Best Parameters for Gradient Boosting:\", gbr_grid_search.best_params_)\n",
    "print(\"Best R2 Score for Gradient Boosting:\", gbr_grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972fdfd",
   "metadata": {},
   "source": [
    "**Evaluate Tunes Gradient Boosting on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c57816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Use the best estimator from GridSearchCV\n",
    "best_gbr_model = gbr_grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "gbr_test_preds = best_gbr_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "r2 = r2_score(y_test, gbr_test_preds)\n",
    "mae = mean_absolute_error(y_test, gbr_test_preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, gbr_test_preds))\n",
    "\n",
    "print(\"Tuned Gradient Boosting Test Set Performance:\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"MAE:      {mae:.4f}\")\n",
    "print(f\"RMSE:     {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875fa70",
   "metadata": {},
   "source": [
    "**Visualize Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcccb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator from grid search\n",
    "best_gbr_model = gbr_grid_search.best_estimator_\n",
    "\n",
    "# Extract feature importances\n",
    "gbr_importances = best_gbr_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create DataFrame\n",
    "gbr_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": gbr_importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=gbr_importance_df.head(15), x=\"Importance\", y=\"Feature\", palette=\"viridis\", hue=\"Feature\", legend=False)\n",
    "plt.title(\"Top 15 Feature Importances - Gradient Boosting Regressor\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"outputs/visuals/gbr_feature_importance_top15.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9acff6",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e62f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ridge Regression - Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "This step performs an exhaustive grid search across different alpha values and solvers \n",
    "to identify the best regularized linear model using 5-fold cross-validation.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Define hyperparameter grid ---\n",
    "ridge_param_grid = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],              # Regularization strength\n",
    "    \"solver\": [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sag\"]  # Different algorithms for optimization\n",
    "}\n",
    "\n",
    "# --- Initialize GridSearchCV ---\n",
    "ridge_grid_search = GridSearchCV(\n",
    "    estimator=Ridge(random_state=42),\n",
    "    param_grid=ridge_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Execute grid search ---\n",
    "ridge_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Output best parameters and cross-validation score ---\n",
    "print(\"Best Parameters for Ridge Regression:\", ridge_grid_search.best_params_)\n",
    "print(\"Best R2 Score for Ridge Regression:\", ridge_grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c2057",
   "metadata": {},
   "source": [
    "**Evaluate Tunes Ridge Regression Performance on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Retrieve the best Ridge model\n",
    "best_ridge_model = ridge_grid_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_ridge = best_ridge_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
    "ridge_mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "ridge_mse = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "# Display results\n",
    "print(\"Tuned Ridge Regression Performance on Test Data:\")\n",
    "print(f\"R2 Score:   {ridge_r2:.4f}\")\n",
    "print(f\"MAE:        {ridge_mae:.4f}\")\n",
    "print(f\"MSE:        {ridge_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90074879",
   "metadata": {},
   "source": [
    "**Visualize Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbba664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "ridge_coef = best_ridge_model.coef_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "ridge_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": np.abs(ridge_coef)\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=ridge_importance_df.head(15), x=\"Importance\", y=\"Feature\", palette=\"viridis\", hue=\"Feature\", legend=False)\n",
    "plt.title(\"Top 15 Feature Importances - Ridge Regressor (via Coefficients)\")\n",
    "plt.xlabel(\"Absolute Coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"outputs/visuals/rr_feature_importance_top15.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41d1b3",
   "metadata": {},
   "source": [
    "### Support Vector Regression(SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda80c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Support Vector Regressor - Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "This step tunes the SVR model by testing combinations of kernel parameters (C, gamma, epsilon)\n",
    "to capture complex relationships in housing data. 5-fold cross-validation ensures stability.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Define hyperparameter grid ---\n",
    "svr_param_grid = {\n",
    "    \"kernel\": [\"rbf\"],              # Radial basis function kernel for non-linear mapping\n",
    "    \"C\": [1, 10, 100],              # Regularization parameter\n",
    "    \"gamma\": [\"scale\", \"auto\"],    # Kernel coefficient\n",
    "    \"epsilon\": [0.1, 0.2, 0.5]      # Insensitivity within margin\n",
    "}\n",
    "\n",
    "# --- Initialize GridSearchCV ---\n",
    "svr_grid_search = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=svr_param_grid,\n",
    "    scoring=\"r2\",\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Execute grid search ---\n",
    "svr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Output best configuration and performance ---\n",
    "print(\"Best Parameters for SVR:\", svr_grid_search.best_params_)\n",
    "print(\"Best R2 Score from Grid Search (CV):\", svr_grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c2795",
   "metadata": {},
   "source": [
    "**Evaluate Tuned SVR Model on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4100c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Retrieve the best SVR model\n",
    "best_svr_model = svr_grid_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_svr = best_svr_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "svr_r2 = r2_score(y_test, y_pred_svr)\n",
    "svr_mae = mean_absolute_error(y_test, y_pred_svr)\n",
    "svr_mse = mean_squared_error(y_test, y_pred_svr)\n",
    "\n",
    "# Display results\n",
    "print(\"Tuned SVR Performance on Test Data:\")\n",
    "print(f\"R2 Score:   {svr_r2:.4f}\")\n",
    "print(f\"MAE:        {svr_mae:.4f}\")\n",
    "print(f\"MSE:        {svr_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146260a",
   "metadata": {},
   "source": [
    "**Visualize Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9265c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance on SVR\n",
    "svr_perm = permutation_importance(best_svr_model, X_test, y_test, n_repeats=30, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create DataFrame\n",
    "svr_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Importance\": svr_perm.importances_mean\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 15\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=svr_importance_df.head(15), x=\"Importance\", y=\"Feature\", palette=\"viridis\", hue=\"Feature\", legend=False)\n",
    "plt.title(\"Top 15 Feature Importances - SVR (via Permutation Importance)\")\n",
    "plt.xlabel(\"Mean Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"outputs/visuals/svr_feature_importance_top15.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481061f",
   "metadata": {},
   "source": [
    "### Tuned Model Performance Observations (Test Set)\n",
    " \n",
    "**Random Forest Regressor**\n",
    "\n",
    "- R2 Score: 0.8766 — Excellent predictive strength, explains nearly 88% of variance.\n",
    "- MAE: 0.1018 — Low average prediction error.\n",
    "- RMSE: 0.1517 — Indicates strong performance in penalizing large errors.\n",
    "\n",
    "Observation: A top performer — accurate, robust, and well-balanced. Ideal for deployment if interpretability isn’t the top priority.\n",
    "\n",
    "**Decision Tree Regressor**\n",
    "\n",
    "- R2 Score: 0.8058 — Decent performance, but lower than ensemble models.\n",
    "- MAE: 0.1373 — Higher error compared to others.\n",
    "- MSE: 0.0362 — Acceptable but suggests overfitting risk.\n",
    "  \n",
    "Observation: Useful for explainability, but lacks generalization. Not ideal for final deployment alone.\n",
    "\n",
    "**Gradient Boosting Regressor**\n",
    "\n",
    "- R2 Score: 0.8774 — Best-in-class R2, slightly edging out Random Forest.\n",
    "- MAE: 0.0998 — Lowest mean error across all models.\n",
    "- RMSE: 0.1513 — Tight error distribution, very stable.\n",
    "\n",
    "Observation: Highest overall performance — suitable for final deployment if latency isn’t a concern. Performs especially well on structured/tabular data.\n",
    "\n",
    "**Ridge Regression**\n",
    "\n",
    "- R2 Score: 0.8686 — Solid performance for a regularized linear model.\n",
    "- MAE: 0.1081 — Slightly higher than tree-based models.\n",
    "- MSE: 0.0245 — Very low squared error.\n",
    "\n",
    "Observation: A strong linear baseline. Efficient, interpretable, and reliable for situations needing quick or explainable predictions.\n",
    "\n",
    "**Support Vector Regressor (SVR)**\n",
    "\n",
    "- R2 Score: 0.7908 — Lowest R2 of the tuned models.\n",
    "- MAE: 0.1189 — Reasonable average error.\n",
    "- MSE: 0.0390 — Moderate squared error.\n",
    "\n",
    "Observation: Performs adequately, but lags behind boosted and ensemble methods. High tuning complexity with limited marginal gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209e882",
   "metadata": {},
   "source": [
    "## Model Comparison & Consolidation\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of this section is to consolidate the performance results from all tuned regression models and visualize them side-by-side using R2, MAE, and RMSE. This comparison provides clarity on which models generalize best to unseen data and serve as the most viable candidates for deployment.\n",
    "\n",
    "We assess each model's predictive power, accuracy, and robustness by evaluating:\n",
    "\n",
    "- R2 Score: How well the model explains variance in the target variable.\n",
    "- MAE: The average magnitude of errors.\n",
    "- RMSE: The standard deviation of residuals, penalizing large errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries after kernel reset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Consolidated model results\n",
    "model_results = [\n",
    "    {\"Model\": \"Random Forest\", \"R2\": 0.8766, \"MAE\": 0.1018, \"RMSE\": 0.1517},\n",
    "    {\"Model\": \"Decision Tree\", \"R2\": dt_r2, \"MAE\": dt_mae, \"RMSE\": dt_rmse},\n",
    "    {\"Model\": \"Gradient Boosting\", \"R2\": 0.8774, \"MAE\": 0.0998, \"RMSE\": 0.1513},\n",
    "    {\"Model\": \"Ridge Regression\", \"R2\": 0.8686, \"MAE\": 0.1081, \"RMSE\": 0.1565},\n",
    "    {\"Model\": \"Support Vector Regressor\", \"R2\": 0.7908, \"MAE\": 0.1189, \"RMSE\": 0.1975}\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# R2 Plot\n",
    "sns.barplot(data=results_df, x=\"R2\", y=\"Model\", palette=\"crest\", ax=axes[0], hue=\"Model\")\n",
    "axes[0].set_title(\"Model Comparison: R2 Score\")\n",
    "axes[0].set_xlim(0.75, 0.90)\n",
    "\n",
    "# MAE Plot\n",
    "sns.barplot(data=results_df, x=\"MAE\", y=\"Model\", palette=\"flare\", ax=axes[1], hue=\"Model\")\n",
    "axes[1].set_title(\"Model Comparison: Mean Absolute Error (MAE)\")\n",
    "\n",
    "# RMSE Plot\n",
    "sns.barplot(data=results_df, x=\"RMSE\", y=\"Model\", palette=\"mako\", ax=axes[2], hue=\"Model\")\n",
    "axes[2].set_title(\"Model Comparison: Root Mean Squared Error (RMSE)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the full figure\n",
    "plot_path = \"outputs/visuals/final_model_comparison.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(\"Consolidated Model Results:\")\n",
    "display(results_df)\n",
    "print(f\"[SAVED] Model comparison plot saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e70ef2",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Gradient Boosting Regressor slightly outperformed others with the highest R2 (0.8774) and lowest RMSE (0.1513), making it the strongest candidate overall.\n",
    "- Random Forest Regressor came very close in performance, offering excellent results with slightly higher RMSE than Gradient Boosting.\n",
    "- Ridge Regression provided strong linear performance (R2: 0.8686), indicating regularized linear models can still compete effectively when features are well-prepared.\n",
    "- Decision Tree Regressor performed reasonably well but showed signs of overfitting compared to ensemble methods.\n",
    "- SVR had the lowest R2 (0.7908), indicating it was less suited to this dataset under current hyperparameters.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Ensemble models like Gradient Boosting and Random Forest consistently outperformed other methods and are recommended for final deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee0602",
   "metadata": {},
   "source": [
    "## Cross-Validation Results (5-Fold)\n",
    "\n",
    "To ensure model robustness and assess generalizability, we performed 5-fold cross-validation on each of the tuned models using the training data.\n",
    "\n",
    "This process helps validate whether the model is overfitting or generalizing well before we evaluate it on the test set.\n",
    "\n",
    "The evaluation metrics include:\n",
    "- **R2 Score**: Measures how well the model explains the variance.\n",
    "- **MAE**: Mean Absolute Error (lower is better).\n",
    "- **RMSE**: Root Mean Squared Error (penalizes large errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define scoring functions\n",
    "scorers = {\n",
    "    \"R2\": \"r2\",\n",
    "    \"MAE\": make_scorer(mean_absolute_error),\n",
    "    \"RMSE\": make_scorer(mean_squared_error, squared=False)\n",
    "}\n",
    "\n",
    "# Dictionary of tuned models\n",
    "tuned_models = {\n",
    "    \"Random Forest\": best_rf_model,\n",
    "    \"Decision Tree\": best_dt_model,\n",
    "    \"Gradient Boosting\": best_gbr_model,\n",
    "    \"Ridge Regression\": best_ridge_model,\n",
    "    \"SVR\": best_svr_model\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "# Evaluate each tuned model with 5-fold CV\n",
    "for name, model in tuned_models.items():\n",
    "    r2 = cross_val_score(model, X_train, y_train, cv=5, scoring=scorers[\"R2\"]).mean()\n",
    "    mae = -cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_mean_absolute_error\").mean()\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\").mean())\n",
    "\n",
    "    cv_results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV R2\": round(r2, 4),\n",
    "        \"CV MAE\": round(mae, 4),\n",
    "        \"CV RMSE\": round(rmse, 4)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "cv_results_df = pd.DataFrame(cv_results).sort_values(by=\"CV R2\", ascending=False)\n",
    "\n",
    "# Display\n",
    "print(\"Cross-Validation Results:\")\n",
    "display(cv_results_df)\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot R2\n",
    "sns.barplot(data=cv_results_df, x=\"CV R2\", y=\"Model\", ax=axes[0], palette=\"crest\", hue=\"Model\", legend=False)\n",
    "axes[0].set_title(\"Cross-Validated R2 Scores\")\n",
    "\n",
    "# Plot MAE\n",
    "sns.barplot(data=cv_results_df, x=\"CV MAE\", y=\"Model\", ax=axes[1], palette=\"flare\", hue=\"Model\", legend=False)\n",
    "axes[1].set_title(\"Cross-Validated MAE\")\n",
    "\n",
    "# Plot RMSE\n",
    "sns.barplot(data=cv_results_df, x=\"CV RMSE\", y=\"Model\", ax=axes[2], palette=\"mako\", hue=\"Model\", legend=False)\n",
    "axes[2].set_title(\"Cross-Validated RMSE\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"outputs/visuals/cross_validation_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43cdd5",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "**Gradient Boosting Regressor demonstrated the strongest overall performance across all metrics:**\n",
    "\n",
    "- Highest CV R2 (0.8756), indicating excellent ability to explain variance in house prices.\n",
    "- Lowest MAE (0.0951) and RMSE (0.1376), suggesting precise and consistent predictions.\n",
    "\n",
    "**Random Forest Regressor was a close second:**\n",
    "\n",
    "- CV R2 of 0.8581 and slightly higher error metrics compared to Gradient Boosting.\n",
    "- A strong, stable model suitable for general use cases where interpretability is less critical.\n",
    "\n",
    "**Ridge Regression performed well for a linear model:**\n",
    "\n",
    "- Achieved CV R2 of 0.8176 with relatively low error.\n",
    "- Its simplicity and resistance to overfitting make it valuable when linearity is assumed or preferred.\n",
    "\n",
    "**Support Vector Regressor (SVR) showed moderate predictive strength:**\n",
    "\n",
    "- Slightly lower CV R2 at 0.7974 and higher MAE/RMSE compared to ensemble models.\n",
    "- Could be improved with feature scaling and advanced kernel tuning.\n",
    "\n",
    "**Decision Tree Regressor was the weakest performer:**\n",
    "\n",
    "- CV R2 of 0.7156, with the highest MAE (0.1470) and RMSE (0.2071).\n",
    "- Prone to overfitting and instability, making it less reliable without pruning or ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1651db",
   "metadata": {},
   "source": [
    "## Test Set Evaluation\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of this section is to evaluate each tuned model's performance on the unseen test set. This helps determine how well the models generalize beyond the training data. By comparing these scores to the cross-validation results, we assess model stability, overfitting risk, and final performance readiness for deployment.\n",
    "\n",
    "We will:\n",
    "- Consolidate the test set R2, MAE, and RMSE for each tuned model.\n",
    "- Compare them across models.\n",
    "- Provide a visual summary for quick interpretation.\n",
    "- Highlight any overfitting or underfitting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after kernel reset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the test results dictionary\n",
    "test_results = {\n",
    "    \"Model\": [\n",
    "        \"Gradient Boosting\", \n",
    "        \"Random Forest\", \n",
    "        \"Ridge Regression\", \n",
    "        \"SVR\", \n",
    "        \"Decision Tree\"\n",
    "    ],\n",
    "    \"Test R2\": [\n",
    "        0.8774, 0.8766, 0.8686, 0.7908, 0.8058\n",
    "    ],\n",
    "    \"Test MAE\": [\n",
    "        0.0998, 0.1018, 0.1081, 0.1189, 0.1373\n",
    "    ],\n",
    "    \"Test RMSE\": [\n",
    "        0.1513, 0.1517, 0.1565, 0.1975, 0.1903\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df_sorted = test_results_df.sort_values(by=\"Test R2\", ascending=False)\n",
    "\n",
    "# Plot comparison for R2, MAE, and RMSE\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "sns.barplot(data=test_results_df_sorted, y=\"Model\", x=\"Test R2\", ax=axes[0], palette=\"crest\", hue=\"Model\", legend=False)\n",
    "axes[0].set_title(\"Test Set R2 Scores\")\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "sns.barplot(data=test_results_df_sorted, y=\"Model\", x=\"Test MAE\", ax=axes[1], palette=\"flare\", hue=\"Model\", legend=False)\n",
    "axes[1].set_title(\"Test Set MAE Scores\")\n",
    "\n",
    "sns.barplot(data=test_results_df_sorted, y=\"Model\", x=\"Test RMSE\", ax=axes[2], palette=\"magma\", hue=\"Model\", legend=False)\n",
    "axes[2].set_title(\"Test Set RMSE Scores\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"outputs/visuals/test_set_model_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24b6c2",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- Gradient Boosting and Random Forest are top performers with R2 scores close to 0.88, indicating excellent model fit and generalization on unseen data.\n",
    "- Gradient Boosting has the lowest MAE (0.0998) and RMSE (0.1513), making it the most accurate and consistent model across all metrics.\n",
    "- Ridge Regression also performs robustly with a high R2 (0.8686), and its lower MAE and RMSE confirm its strong linear modeling capability.\n",
    "- SVR and Decision Tree trail behind in all three metrics, with higher errors and comparatively lower R2 values, suggesting limited capacity to generalize complex price patterns.\n",
    "\n",
    "**Recommendation:** \n",
    "\n",
    "- Gradient Boosting should be the preferred model moving forward for deployment.\n",
    "- The Gradient Boosting model achieved an R2 of 0.8774 on unseen test data, clearly meeting the project’s performance goal of R2 ≥ 0.75 and validating its suitability for final deployment.\n",
    "- These results confirm that the selected model generalizes well to unseen data and meets the original business objective of predicting fair property prices with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c4cb0",
   "metadata": {},
   "source": [
    "## Residual Analysis\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Residual analysis helps assess the performance of regression models by visualizing and interpreting the difference between predicted and actual values (residuals). This is crucial to:\n",
    "- Identify non-random patterns or bias.\n",
    "- Detect heteroscedasticity (changing variance across predictions).\n",
    "- Ensure model assumptions (e.g., normality, independence) are not violated.\n",
    "- Spot influential outliers that might distort model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d57408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of tuned models\n",
    "tuned_models = {\n",
    "    \"Random Forest\": best_rf_model,\n",
    "    \"Decision Tree\": best_dt_model,\n",
    "    \"Gradient Boosting\": best_gbr_model,\n",
    "    \"Ridge Regression\": best_ridge_model,\n",
    "    \"SVR\": best_svr_model\n",
    "}\n",
    "\n",
    "# Plot residual distributions\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for idx, (name, model) in enumerate(tuned_models.items(), 1):\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    plt.subplot(3, 2, idx)\n",
    "    sns.histplot(residuals, kde=True, bins=30)\n",
    "    plt.title(f\"Residual Distribution - {name}\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"outputs/visuals/residual_distribution.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7cf54e",
   "metadata": {},
   "source": [
    "**Plot: Residuals vs. Predicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals vs predicted values\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for idx, (name, model) in enumerate(tuned_models.items(), 1):\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    plt.subplot(3, 2, idx)\n",
    "    sns.scatterplot(x=y_pred, y=residuals, alpha=0.6)\n",
    "    plt.axhline(0, linestyle='--', color='black', linewidth=1)\n",
    "    plt.title(f\"Residuals vs Predicted - {name}\")\n",
    "    plt.xlabel(\"Predicted Log(SalePrice)\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"outputs/visuals/residual_vs_predicted.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae04ab9",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- Random Forest & Gradient Boosting showed tight, symmetrical residual distributions with minimal bias — validating their superior R2 and RMSE scores.\n",
    "- Ridge Regression produced reasonably centered residuals but had slightly longer tails, suggesting some underfitting on complex patterns.\n",
    "- Decision Tree had higher error variance and showed signs of overfitting with outliers.\n",
    "- SVR had wider scatter and some bias near the prediction extremes, confirming its slightly lower test R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c2a11",
   "metadata": {},
   "source": [
    "## Feature Importance & Insights\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this section, we aim to identify which features have the greatest influence on house price predictions. Understanding feature importance helps:\n",
    "\n",
    "- Interpret model decisions\n",
    "- Guide further feature engineering\n",
    "- Offer actionable insights for stakeholders (e.g., real estate clients)\n",
    "\n",
    "We will analyze feature importance from Random Forest and Decision Tree models — both tree-based models that naturally compute feature contribution based on information gain.\n",
    "\n",
    "We chose these models because they are:\n",
    "\n",
    "- Intrinsically interpretable\n",
    "- Robust for mixed feature types\n",
    "- Aligned with the structure and complexity of real estate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96cbc6e",
   "metadata": {},
   "source": [
    "**Random Forest Regressor: Feature Importance**\n",
    "\n",
    "We reuse the previously generated feature importance plot for the Random Forest model to anchor the comparative insights in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Top 15 Feature Importances - Random Forest (Reused)\")\n",
    "display(Image(\"outputs/visuals/rf_feature_importance_top15.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7345c",
   "metadata": {},
   "source": [
    "**Decision Tree Regressor: Feature Importance**\n",
    "\n",
    "We reference the previously saved plot to avoid redundancy. The Decision Tree model highlights different splits, providing contrast to the Random Forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Top 15 Feature Importances - Decision Tree (Reused)\")\n",
    "display(Image(\"outputs/visuals/dt_feature_importance_top15.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd8604",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- Overall Quality (num__OverallQual) ranked highest in both Random Forest and Decision Tree models, reinforcing its critical influence on sale prices.\n",
    "- Above Ground Living Area (num__GrLivArea) and Garage Area (num__GarageArea) consistently showed high importance — supporting earlier correlation and residual analyses.\n",
    "- Features like Finished Basement Ratio, Lot Area, and YearBuilt had medium to high importance, highlighting structure age and layout as price influencers.\n",
    "- Low-importance features may still hold interaction effects (to be tested via SHAP/partial dependence next)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d479f4",
   "metadata": {},
   "source": [
    "## Model Explainability\n",
    "\n",
    "### (Random Forest) Objective\n",
    "\n",
    "The goal of this section is to understand how the Random Forest model arrives at its predictions. This builds transparency and trust in the model’s behavior — especially for non-technical stakeholders. \n",
    "\n",
    "**We’ll focus on:**\n",
    "\n",
    "- Identifying the most influential features globally.\n",
    "- Understanding feature impact on individual predictions using SHAP (SHapley Additive exPlanations).\n",
    "- Random Forests support intrinsic feature importance, and SHAP adds explainability at the individual prediction level.\n",
    "\n",
    "**We will:**\n",
    "\n",
    "- Revisit and visualize feature importances for both models\n",
    "- Apply SHAP (SHapley Additive exPlanations) for deeper interpretability:\n",
    "- Global interpretation: Understand which features drive predictions overall\n",
    "- Local interpretation: Understand why the model made a specific prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a3670",
   "metadata": {},
   "source": [
    "**Import Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e76854",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c379f",
   "metadata": {},
   "source": [
    "**SHAP Summary Plot for Random Forest (Global Interpretability)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544825f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SHAP Analysis for Random Forest Model\n",
    "\n",
    "This script calculates and visualizes SHAP (SHapley Additive exPlanations) values\n",
    "to explain the feature contributions for predictions made by the tuned Random Forest model.\n",
    "\"\"\"\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Check and fill missing values in test/train sets ---\n",
    "print(\"Checking for nulls in input datasets (Random Forest SHAP)...\")\n",
    "print(\"X_train null values:\\n\", X_train.isnull().sum().sort_values(ascending=False).head())\n",
    "\n",
    "# Use mean imputation for missing numeric values to avoid SHAP errors\n",
    "X_train_filled = X_train.fillna(X_train.mean())\n",
    "X_test_filled = X_test.fillna(X_train.mean())  # use training mean to prevent data leakage\n",
    "\n",
    "# --- Initialize Tree Explainer for Random Forest ---\n",
    "print(\"Initializing SHAP explainer (Random Forest)...\")\n",
    "explainer_rf = shap.TreeExplainer(best_rf_model)\n",
    "\n",
    "# --- Calculate SHAP values on test set ---\n",
    "print(\"Calculating SHAP values for X_test (Random Forest)...\")\n",
    "shap_values_rf = explainer_rf.shap_values(X_test_filled)\n",
    "\n",
    "# --- Summary Plot: Mean absolute SHAP values per feature ---\n",
    "print(\"Plotting SHAP summary plot (bar)...\")\n",
    "shap.summary_plot(shap_values_rf, X_test_filled, plot_type=\"bar\", show=True)\n",
    "\n",
    "# --- Summary Plot: Detailed dot plot of SHAP values ---\n",
    "print(\"Plotting SHAP summary plot (dot)...\")\n",
    "shap.summary_plot(shap_values_rf, X_test_filled, show=True)\n",
    "\n",
    "# --- Create SHAP contribution table ---\n",
    "print(\"Computing average SHAP contributions...\")\n",
    "shap_rf_df = pd.DataFrame(shap_values_rf, columns=X_test_filled.columns)\n",
    "shap_rf_mean = shap_rf_df.abs().mean(axis=0).sort_values(ascending=False)\n",
    "shap_rf_table = pd.DataFrame({\n",
    "    \"Feature\": shap_rf_mean.index,\n",
    "    \"Mean SHAP Value\": shap_rf_mean.values\n",
    "})\n",
    "\n",
    "# Display top contributing features\n",
    "print(\"Top 10 Features by SHAP Contribution:\")\n",
    "print(shap_rf_table.head(10))\n",
    "\n",
    "# --- Generate SHAP dependence plot for top feature ---\n",
    "print(\"Generating SHAP dependence plot for top feature...\")\n",
    "top_feature = shap_rf_table[\"Feature\"].iloc[0]\n",
    "shap.dependence_plot(\n",
    "    top_feature,\n",
    "    shap_values_rf,\n",
    "    X_test_filled,\n",
    "    interaction_index=None,\n",
    "    show=True\n",
    ")\n",
    "\n",
    "# Save dependence plot\n",
    "plt.gcf().tight_layout()\n",
    "plt.savefig(f\"outputs/visuals/shap_rf_dependence_{top_feature}.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e2922",
   "metadata": {},
   "source": [
    "### (XGBoost) Objective\n",
    "\n",
    "To understand how XGBoost makes predictions by analyzing feature-level contributions using SHAP (SHapley Additive exPlanations). This enhances model transparency and supports interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SHAP Explainability for Gradient Boosting Regressor (GBR)\n",
    "\n",
    "This script uses SHAP (SHapley Additive exPlanations) to evaluate how each feature \n",
    "influences the model’s predictions. It generates feature importance tables, global summary plots, \n",
    "and a dependence plot for the most influential feature.\n",
    "\"\"\"\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Initialize SHAP Explainer for Gradient Boosting ---\n",
    "explainer_gbr = shap.Explainer(best_gbr_model)\n",
    "\n",
    "# --- Compute SHAP values on the test set ---\n",
    "shap_values_gbr = explainer_gbr(X_test_filled)\n",
    "\n",
    "# --- Mean absolute SHAP value per feature for ranking ---\n",
    "shap_mean_gbr = np.abs(shap_values_gbr.values).mean(axis=0)\n",
    "\n",
    "# --- Create DataFrame with SHAP importance ---\n",
    "shap_importance_gbr_df = pd.DataFrame({\n",
    "    \"Feature\": X_test_filled.columns,\n",
    "    \"Mean SHAP Value\": shap_mean_gbr\n",
    "}).sort_values(by=\"Mean SHAP Value\", ascending=False)\n",
    "\n",
    "# --- Display Top 10 Features ---\n",
    "print(\"Top 10 Contributing Features (XGBoost SHAP):\")\n",
    "display(shap_importance_gbr_df.head(10))\n",
    "\n",
    "# --- SHAP Summary Plots ---\n",
    "print(\"Generating SHAP summary plots (bar and beeswarm)...\")\n",
    "shap.summary_plot(shap_values_gbr, X_test_filled, plot_type=\"bar\", show=True)\n",
    "shap.summary_plot(shap_values_gbr, X_test_filled, show=True)\n",
    "\n",
    "# --- Dependence Plot for Top Feature ---\n",
    "print(\"Generating SHAP dependence plot for top-ranked feature...\")\n",
    "top_feature_gbr = X_test_filled.columns[np.argsort(shap_mean_gbr)[-1]]  # top feature by SHAP\n",
    "\n",
    "shap.dependence_plot(\n",
    "    top_feature_gbr,\n",
    "    shap_values_gbr.values,\n",
    "    X_test_filled,\n",
    "    interaction_index=None,\n",
    "    show=True\n",
    ")\n",
    "\n",
    "# Save the dependence plot\n",
    "plt.gcf().tight_layout()\n",
    "plt.savefig(f\"outputs/visuals/shap_gbr_dependence_{top_feature_gbr}.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f5f7d",
   "metadata": {},
   "source": [
    "#### SHAP Feature Contribution Observations:\n",
    "\n",
    "The SHAP value analysis for both XGBoost and Random Forest reveals strong consensus on the most influential features driving house price predictions. Here's a breakdown of key observations:\n",
    "\n",
    "**Common Top Contributors**\n",
    "\n",
    "1. overallqual (Overall Quality):\n",
    "\n",
    "- Ranked #1 in both models.\n",
    "- It shows the highest SHAP value across the board, confirming it's the most decisive factor in predicting sale price.\n",
    "\n",
    "Observation: Property quality is perceived and priced as a premium attribute, reaffirming earlier correlation and importance plots.\n",
    "\n",
    "2. grlivarea (Above-Ground Living Area):\n",
    "\n",
    "- Consistently ranked #2, with strong SHAP values.\n",
    "\n",
    "Observation: Larger, usable living areas have direct positive impact on value perception and final pricing.\n",
    "\n",
    "3. totalbsmtsf, bsmtfinsf1 (Basement Area & Finish):\n",
    "\n",
    "- Appear in top 5 in both models.\n",
    "\n",
    "Observation: Finished and spacious basements add perceived livability and investment value.\n",
    "\n",
    "4. yearbuilt & yearremodadd:\n",
    "\n",
    "- Reflects recency of construction or upgrades, appearing in top 10 consistently.\n",
    "\n",
    "Observation: Modern or recently renovated homes command higher prices due to reduced maintenance risk and updated features.\n",
    "\n",
    "5. garagearea:\n",
    "\n",
    "- Present in both lists.\n",
    "\n",
    "Observation: Functional garages are a valuable utility space, especially in suburban residential markets.\n",
    "\n",
    "**Model-Specific Insights**\n",
    "\n",
    "- Random Forest gives slightly more weight to overallqual, while XGBoost distributes importance a bit more evenly across additional features like overallcond, lotarea, and 1stflrsf.\n",
    "- overallcond is ranked higher by XGBoost than Random Forest, suggesting it captures more subtle condition nuances XGBoost may handle better.\n",
    "\n",
    "#### Final Takeaway\n",
    "- These insights validate our earlier EDA conclusions — confirming that quality, size, and condition are core price drivers.\n",
    "- SHAP strengthens our model explainability, offering transparency and interpretability critical for client trust and business deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf1635",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "We will now retain the trained models, processed data, and pertinent outputs, like evaluation metrics and feature importance statistics, to guarantee reproducibility and make additional analysis easier. These saved files can be used in notebooks or projects in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2db560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Define output directories\n",
    "metrics_path = \"outputs/metrics\"\n",
    "feature_importance_path = \"outputs/ft_importance\"\n",
    "shap_values_path = \"outputs/shap_values\"\n",
    "processed_data_path = \"data/processed/final\"\n",
    "models_dir = \"outputs/models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(metrics_path, exist_ok=True)\n",
    "os.makedirs(feature_importance_path, exist_ok=True)\n",
    "os.makedirs(shap_values_path, exist_ok=True)\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# --- Save Evaluation Metrics ---\n",
    "results_df.to_csv(f\"{metrics_path}/consolidated_model_performance.csv\", index=False)\n",
    "cv_results_df.to_csv(f\"{metrics_path}/cross_validation_results.csv\", index=False)\n",
    "test_results_df.to_csv(f\"{metrics_path}/test_set_results.csv\", index=False)\n",
    "\n",
    "# --- Save Feature Importances ---\n",
    "importance_df.to_csv(f\"{feature_importance_path}/random_forest_importance.csv\", index=False)\n",
    "gbr_importance_df.to_csv(f\"{feature_importance_path}/xgboost_importance.csv\", index=False)\n",
    "\n",
    "# --- Save SHAP Summary Tables ---\n",
    "shap_rf_table.to_csv(f\"{shap_values_path}/shap_summary_random_forest.csv\", index=False)\n",
    "shap_importance_gbr_df.to_csv(f\"{shap_values_path}/shap_summary_xgboost.csv\", index=False)\n",
    "\n",
    "# --- Save Final Datasets ---\n",
    "X_train.to_csv(f\"{processed_data_path}/X_train.csv\", index=False)\n",
    "X_test.to_csv(f\"{processed_data_path}/X_test.csv\", index=False)\n",
    "pd.Series(y_train).to_csv(f\"{processed_data_path}/y_train.csv\", index=False)\n",
    "pd.Series(y_test).to_csv(f\"{processed_data_path}/y_test.csv\", index=False)\n",
    "\n",
    "# --- Save Tuned Models ---\n",
    "joblib.dump(best_rf_model, f\"{models_dir}/best_random_forest.pkl\")\n",
    "joblib.dump(best_dt_model, f\"{models_dir}/best_decision_tree.pkl\")\n",
    "joblib.dump(best_gbr_model, f\"{models_dir}/best_gradient_boosting.pkl\")\n",
    "joblib.dump(best_ridge_model, f\"{models_dir}/best_ridge.pkl\")\n",
    "joblib.dump(best_svr_model, f\"{models_dir}/best_svr.pkl\")\n",
    "\n",
    "try:\n",
    "    # All your saving logic here\n",
    "\n",
    "    print(\"All key outputs and trained models saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Something went wrong: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cf6d4",
   "metadata": {},
   "source": [
    "## Next Steps: \n",
    "\n",
    "### Transition to Final Pipeline & Deployment\n",
    "\n",
    "With the completion of model training, evaluation, interpretability analysis, and final output saving, we are now ready to proceed with the next stage of the project:\n",
    "\n",
    "**1. Final Pipeline Construction**\n",
    "\n",
    "- Build a unified pipeline integrating preprocessing and the best-performing model.\n",
    "- Ensure it handles real-world inputs and edge cases gracefully.\n",
    "- Validate with unseen/inherited data.\n",
    "\n",
    "**2. Model and Artifact Serialization**\n",
    "\n",
    "- Use joblib to save the entire pipeline object for reuse.\n",
    "- Save supporting artefacts: scalers, encoders, column orders, and metadata.\n",
    "\n",
    "**3. Deployment Preparation**\n",
    "\n",
    "- Develop a prediction function that takes user input and returns a clean prediction with interpretations.\n",
    "- Prepare for either:\n",
    "\n",
    "    - Jupyter dashboard (streamlit/voila)\n",
    "    - Flask/Django API setup (if desired)\n",
    "\n",
    "**4. Documentation**\n",
    "\n",
    "- Summarize business insights, technical implementation, and outcomes.\n",
    "- Prepare README and technical appendix for Code Institute submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
