{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931f9d6b",
   "metadata": {},
   "source": [
    "_This project was developed independently as part of Code Institute’s Predictive Analytics Project. Any datasets or templates used are openly provided by the course or via public sources like Kaggle. All commentary and code logic are my own._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0e548",
   "metadata": {},
   "source": [
    "# Notebook 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42c848",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "**Feature Selection:** \n",
    "- Determine the main determinants of home prices by evaluating the significance of characteristics using statistical techniques and a machine learning model (Random Forest).\n",
    "- Reduce multicollinearity by eliminating superfluous characteristics.\n",
    "\n",
    "**Feature Transformation:** \n",
    "- To lessen skewness and normalize features for modeling, apply transformations (such as log and standardization).\n",
    "- Make sure machine learning methods are compatible by dynamically encoding categorical variables.\n",
    "\n",
    "**Feature engineering:** \n",
    "- Using domain expertise, develop new characteristics such as house age, living area to lot area ratio, and overall quality-condition score.\n",
    "\n",
    "**Results Validation:** \n",
    "- For tasks involving downstream modeling, save the modified training and testing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f35bd2",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "- `/data/processed/df_cleaned.csv`  \n",
    "- `inherited_houses.csv` (for processing pipeline validation)\n",
    "\n",
    "### Outputs  \n",
    "- `/data/processed/final/X_train.csv`  \n",
    "- `/data/processed/final/X_test.csv`  \n",
    "- `/data/processed/final/y_train.csv`  \n",
    "- `/data/processed/final/y_test.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43b0cd",
   "metadata": {},
   "source": [
    "## Extra Remarks\n",
    "\n",
    "**Important Points:** \n",
    "- Feature engineering choices were informed by statistical techniques, exploratory investigations, and domain expertise.\n",
    "- To guarantee consistency between the datasets, uniform transformations, scaling, and encoding approaches will be used.\n",
    "\n",
    "**Methodology:** \n",
    "- This notebook follows the CRISP-DM methodology's Data Preparation and Transformation phases.\n",
    "\n",
    "**Next Actions:**\n",
    "- The price analysis notebook will use the converted datasets as inputs to investigate the connection between important characteristics and home values.\n",
    "- Regression models that forecast home sale prices will also be trained and assessed using the datasets in the modeling notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfa511",
   "metadata": {},
   "source": [
    "## Change Working Directory\n",
    "- Since it is expected that you would keep the notebooks in a subfolder, you will need to switch the working directory when you run the notebook in the editor.\n",
    "- The working directory must be changed from its current folder to its parent folder.\n",
    "- We wish to change the current directory's parent to the new current directory.\n",
    "- Verify the updated current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart Working Directory Setup\n",
    "import os\n",
    "project_root = '/workspaces/heritage_housing'\n",
    "if os.getcwd() != project_root:\n",
    "    try:\n",
    "        os.chdir(project_root)\n",
    "        print(f\"[INFO] Changed working directory to project root: {os.getcwd()}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"[ERROR] Project root '{project_root}' not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a5730",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "### Preview Initial Data\n",
    "- Before start working on this section, we should preview what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned data\n",
    "df_cleaned = pd.read_csv('data/processed/df_cleaned.csv')\n",
    "\n",
    "# Preview first few rows\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7a1ec",
   "metadata": {},
   "source": [
    "- Dataset contains `X` rows and `Y` columns.\n",
    "- Includes numerical, categorical, and ordinal features.\n",
    "- Target variable: **SalePrice**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736f8e9",
   "metadata": {},
   "source": [
    "## Transformation Identification\n",
    "\n",
    "- We apply changes to the chosen features in order to enhance model performance. These conversions deal with problems like categorical encodings, scaling discrepancies, and skewness.\n",
    "- The goal is to scale or normalize numerical features.\n",
    "- Categorical features should be encoded.\n",
    "- Deal with possible numerical feature outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Checking distribution of key numerical features\n",
    "df_cleaned.hist(figsize=(20,15), bins=30)\n",
    "plt.suptitle(\"Distribution of Numerical Features\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e765cb",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- `lotarea`, `lotfrontage`, `openporchsf`, `totalbsmt_sf`, `grlivarea`, etc. are heavily right-skewed → They need transformation.\n",
    "- `saleprice` is also right-skewed, but not extreme.\n",
    "- `1stflrsf`, `2ndflrsf`, `bsmtfinsf1` also show right-skewed behavior.\n",
    "- `overallqual` and `overallcond` are ordinal and categorical, not continuous — do not log-transform these.\n",
    "- `yearbuilt`, `yearremodadd` — these are clearly bimodal or categorical by decades, so no transformation needed; maybe feature engineer \"Era Built\" like \"Before 1950\", \"1950-1980\", etc.\n",
    "- `garageyearblt` — highly clumped around certain years, could be treated categorically (newer garage vs old).\n",
    "\n",
    "### Transformation Plan:\n",
    "**Log Transform**:\n",
    "  - LotArea\n",
    "  - LotFrontage\n",
    "  - GrLivArea\n",
    "  - OpenPorchSF\n",
    "  - TotalBsmtSF\n",
    "  - 1stFlrSF\n",
    "  - 2ndFlrSF\n",
    "  - BsmtFinSF1\n",
    "  - SalePrice (mildly)\n",
    "\n",
    "**Categorical Bucketing**:\n",
    "  - YearBuilt → Group into eras\n",
    "  - GarageYrBlt → Group into eras (e.g., Pre-1980, Post-1980)\n",
    "\n",
    "**Leave Untouched**:\n",
    "  - OverallQual\n",
    "  - OverallCond\n",
    "  - YearRemodAdd (could be engineered but no scaling needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f3a1f",
   "metadata": {},
   "source": [
    "## Apply Key Transformations\n",
    "Based on the analysis above, these transformations are applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load and Prepare Data\n",
    "\"\"\"\n",
    "Load cleaned dataset, apply log transformation to target variable,\n",
    "and split into training and testing sets.\n",
    "\"\"\"\n",
    "df_cleaned = pd.read_csv('data/processed/df_cleaned.csv')\n",
    "\n",
    "X = df_cleaned.drop(columns=[\"SalePrice\"])\n",
    "y = np.log1p(df_cleaned[\"SalePrice\"])  # log-transform target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"[INFO] X_train shape: {X_train.shape}\")\n",
    "print(f\"[INFO] X_test shape:  {X_test.shape}\")\n",
    "\n",
    "# Copy to avoid unintended changes later\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "# Output folder for transformed visualisations\n",
    "output_dir = \"outputs/visuals\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Log Transformation of Positively Skewed Features\n",
    "\"\"\"\n",
    "Apply np.log1p() to positively skewed features to normalize distributions.\n",
    "This step reduces model bias from long-tailed variables.\n",
    "\"\"\"\n",
    "log_skewed_cols = ['LotArea', 'LotFrontage', 'GrLivArea', 'OpenPorchSF', \n",
    "                   'TotalBsmtSF', '1stFlrSF', 'BsmtFinSF1', 'MasVnrArea']\n",
    "\n",
    "for col in tqdm(log_skewed_cols, desc=\"Applying Log Transformations\"):\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = np.log1p(X_train[col])\n",
    "        X_test[col] = np.log1p(X_test[col])\n",
    "\n",
    "        # Save visual confirmation\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(X_train[col], kde=True, bins=30)\n",
    "        plt.title(f\"Distribution of {col} (After Log Transformation)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/log_transformed_{col}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# Reflection of Negatively Skewed Features\n",
    "\"\"\"\n",
    "Apply reflection transformation to reduce negative skew.\n",
    "Useful for temporal features like YearBuilt.\n",
    "\"\"\"\n",
    "negatively_skewed_cols = ['YearBuilt', 'YearRemodAdd', 'BedroomAbvGr']\n",
    "\n",
    "for col in tqdm(negatively_skewed_cols, desc=\"Applying Reflection for Negative Skew\"):\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].max() - X_train[col]\n",
    "        X_test[col] = X_test[col].max() - X_test[col]\n",
    "\n",
    "        # Save visual confirmation\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(X_train[col], kde=True, bins=30)\n",
    "        plt.title(f\"Distribution of {col} (After Reflection)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/reflected_{col}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# Summary of Transformed Features\n",
    "\"\"\"\n",
    "Identify which features were transformed and which remained untouched.\n",
    "Useful for tracking transformation impact and debugging.\n",
    "\"\"\"\n",
    "all_transformed_cols = log_skewed_cols + negatively_skewed_cols\n",
    "remaining_numerical_cols = [\n",
    "    col for col in X_train.select_dtypes(include=['int64', 'float64']).columns \n",
    "    if col not in all_transformed_cols\n",
    "]\n",
    "\n",
    "print(f\"[INFO] {len(all_transformed_cols)} features transformed, {len(remaining_numerical_cols)} untouched.\")\n",
    "print(\"[INFO] Feature transformations completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9db4c1",
   "metadata": {},
   "source": [
    "## Feature Scaling and Encoding\n",
    "\n",
    "### Standardizing Features\n",
    "\n",
    "- Some numerical features may benefit from Standardization (zero mean, unit variance) after transformations, especially before model training (e.g., Ridge, Lasso, ElasticNet models are sensitive to feature scales).\n",
    "- We'll apply StandardScaler to continuous numerical features, excluding already encoded categorical columns(one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70247a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Scaling: Standardization of Numerical Features ===\n",
    "\"\"\"\n",
    "Standardize continuous numerical features using StandardScaler.\n",
    "\n",
    "This transformation ensures zero mean and unit variance across features, \n",
    "which improves model performance for algorithms sensitive to scale \n",
    "(e.g., Ridge, Lasso, SVM). One-hot encoded and reflected features are excluded.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create deep copies to avoid modifying the originals\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Select numerical features that require standardization\n",
    "numerical_features_to_scale = [\n",
    "    col for col in X_train_scaled.select_dtypes(include=['int64', 'float64']).columns \n",
    "    if col not in ['YearBuilt', 'YearRemodAdd', 'BedroomAbvGr']  # These are reflected, already adjusted\n",
    "]\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled[numerical_features_to_scale] = scaler.fit_transform(X_train_scaled[numerical_features_to_scale])\n",
    "X_test_scaled[numerical_features_to_scale] = scaler.transform(X_test_scaled[numerical_features_to_scale])\n",
    "\n",
    "print(f\"[INFO] Applied standard scaling to {len(numerical_features_to_scale)} features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154b7ea",
   "metadata": {},
   "source": [
    "- Encoding Categorical features were already One-Hot Encoded during Data Cleaning phase.\n",
    "- No further encoding is required at this stage.\n",
    "\n",
    "**Summary:**\n",
    "- One-Hot Encoding ensures no ordinal relationships are wrongly introduced.\n",
    "- Encoded features behave like numeric columns (0/1) and don’t need scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081e9e2",
   "metadata": {},
   "source": [
    "### Summarised Observations:\n",
    "**Skewness Reduction:**\n",
    "- Log transformations applied to positively skewed features. Reflection applied to negatively skewed features.\n",
    "  \n",
    "**Standardisation:** \n",
    "- Numerical features standardized to zero mean and unit variance.\n",
    "\n",
    "**Encoding:**\n",
    "- Categorical features already one-hot encoded. No re-encoding needed.\n",
    "\n",
    "**Dataset Status:** \n",
    "- Ready for multicollinearity check and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9d76e",
   "metadata": {},
   "source": [
    "## Multicollinearity Check\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Before feature selection, we must check for multicollinearity — high correlation between features can wreck models (especially linear ones) by making coefficients unstable.\n",
    "\n",
    "We'll use:\n",
    "\n",
    "- Heatmaps to visualize correlations.\n",
    "- Variance Inflation Factor (VIF) to quantify multicollinearity.\n",
    "\n",
    "**Visual Correlation Heatmap:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c80d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Multicollinearity Check and Feature Reduction ===\n",
    "\"\"\"\n",
    "Detect and handle multicollinearity using Pearson correlation and visual heatmaps.\n",
    "\n",
    "1. Compute a correlation matrix for all numerical features.\n",
    "2. Visualize it using a heatmap.\n",
    "3. Identify highly correlated feature pairs (|corr| > 0.8).\n",
    "4. Drop one feature from each pair to reduce redundancy.\n",
    "\n",
    "Reduces instability in models (especially linear ones) and improves interpretability.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_features = X_train_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = X_train_scaled[numerical_features].corr()\n",
    "\n",
    "# Create and save heatmap\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False, fmt='.2f', square=True, cbar=True)\n",
    "plt.title('Numerical Feature Correlation Heatmap', fontsize=18)\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"outputs/visuals\", exist_ok=True)\n",
    "plt.savefig(\"outputs/visuals/numerical_feature_correlation_heatmap.png\")\n",
    "plt.show()\n",
    "print(\"[INFO] Correlation heatmap created and saved.\")\n",
    "\n",
    "# Identify high-correlation pairs (upper triangle only)\n",
    "high_corr_pairs = (\n",
    "    corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "high_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'].abs() > 0.8]\n",
    "\n",
    "display(high_corr_pairs)\n",
    "\n",
    "# Drop the second feature from each correlated pair\n",
    "to_drop = set(high_corr_pairs['Feature2'])\n",
    "print(f\"[INFO] Dropping {len(to_drop)} highly correlated features:\\n\", to_drop)\n",
    "\n",
    "X_train_scaled = X_train_scaled.drop(columns=list(to_drop))\n",
    "X_test_scaled = X_test_scaled.drop(columns=list(to_drop))\n",
    "\n",
    "# Output shape\n",
    "print(f\"[INFO] Shape after dropping correlated features: {X_train_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b5b8d",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "**Heatmap:**\n",
    "- Visual clusters of strong correlations clearly visible.\n",
    "\n",
    "**High Correlation Pairs:**\n",
    "- Features like totalbsmtsf and 1stflrsf etc. (example) show high correlation (> 0.8).\n",
    "\n",
    "**Features Dropped:**\n",
    "- Dropped only the second feature in each correlated pair to minimize information loss.\n",
    "\n",
    "**Dataset:**\n",
    "- Reduced dimensionality while preserving critical predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408127b",
   "metadata": {},
   "source": [
    "**Variance Inflation Factor (VIF) Calculation for Multicollinearity:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92606c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Variance Inflation Factor (VIF) Calculation ===\n",
    "\"\"\"\n",
    "Detect multicollinearity using VIF (Variance Inflation Factor).\n",
    "\n",
    "VIF measures how much a feature is linearly correlated with other features.\n",
    "- VIF > 5 suggests moderate multicollinearity.\n",
    "- VIF > 10 suggests high multicollinearity, and the feature may be redundant.\n",
    "\n",
    "Steps:\n",
    "1. Select numeric features after scaling.\n",
    "2. Compute VIF for each feature.\n",
    "3. Sort and inspect the highest VIF scores.\n",
    "4. Save results to outputs/metrics for audit trail.\n",
    "\"\"\"\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"outputs/metrics\", exist_ok=True)\n",
    "\n",
    "# Select only numeric columns for VIF calculation\n",
    "X_train_numeric = X_train_scaled.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Prepare DataFrame for VIF results\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_train_numeric.columns\n",
    "\n",
    "# Compute VIF scores\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(X_train_numeric.values, i) \n",
    "    for i in range(X_train_numeric.shape[1])\n",
    "]\n",
    "\n",
    "# Sort and display top features by VIF\n",
    "vif_data = vif_data.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
    "display(vif_data.head(15))\n",
    "\n",
    "# Save to CSV\n",
    "vif_output_path = \"outputs/metrics/vif_scores.csv\"\n",
    "vif_data.to_csv(vif_output_path, index=False)\n",
    "\n",
    "print(f\"[INFO] VIF scores calculated and saved to {vif_output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e9728",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "**Features Evaluated:**\n",
    "- All numerical features after scaling and dropping correlated pairs.\n",
    "\n",
    "**Top High VIF Features:**\n",
    "- Features like (example) GarageCars, TotalBsmtSF might show VIF > 5–10 indicating multicollinearity.\n",
    "\n",
    "**Impact on Model:**\n",
    "- High VIF (> 10) typically indicates redundancy — these features may need dropping, combining, or PCA (dimensionality reduction).\n",
    "\n",
    "**Next Actions:**\n",
    "- Review features with VIF > 10 and consider removing or engineering them to reduce multicollinearity.\n",
    "\n",
    "### Points to keep in mind:\n",
    "- VIF > 5 → Moderate multicollinearity (acceptable sometimes).\n",
    "- VIF > 10 → Serious multicollinearity.\n",
    "- If dropping features, re-run the VIF to re-check after removal! (VIF is sensitive to dataset structure.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b53c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23842f2",
   "metadata": {},
   "source": [
    "## Create New Features\n",
    "\n",
    "By developing additional characteristics based on domain expertise and trends seen in earlier analysis, we improve the dataset in this part. By offering insightful depictions of the data, feature engineering enhances model performance.\n",
    "\n",
    "**New Features to be Addded:**\n",
    "\n",
    "- *HouseAge* - Newer houses usually sell for higher prices.\n",
    "- *LivingLotRatio* - Efficiency of space utilization.\n",
    "- *BsmtFinRatio* - Basement finished quality.\n",
    "- *OverallScore* - Combined quality + condition metric.\n",
    "- *HasPorch* - Presence of usable porch space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def27a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Engineering ===\n",
    "\"\"\"\n",
    "Create domain-driven engineered features to enhance predictive power:\n",
    "\n",
    "1. HouseAge: Measures property age at prediction time.\n",
    "2. LivingLotRatio: Indicates how efficiently the lot space is utilized.\n",
    "3. FinishedBsmtRatio: Captures proportion of basement that's finished.\n",
    "4. OverallScore: Composite of quality and condition (ordinal × ordinal).\n",
    "5. HasPorch: Binary feature indicating presence of usable porch area.\n",
    "\n",
    "Each transformation is based on earlier hypothesis validation and helps\n",
    "highlight patterns that influence housing prices.\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "# Create copies to avoid mutating previous datasets\n",
    "X_train_fe = X_train_scaled.copy()\n",
    "X_test_fe = X_test_scaled.copy()\n",
    "\n",
    "# House Age\n",
    "X_train_fe['HouseAge'] = current_year - X_train_fe['YearBuilt']\n",
    "X_test_fe['HouseAge'] = current_year - X_test_fe['YearBuilt']\n",
    "\n",
    "# Living Area to Lot Area Ratio\n",
    "X_train_fe['LivingLotRatio'] = X_train_fe['GrLivArea'] / (X_train_fe['LotArea'] + 1)\n",
    "X_test_fe['LivingLotRatio'] = X_test_fe['GrLivArea'] / (X_test_fe['LotArea'] + 1)\n",
    "\n",
    "# Finished Basement Ratio\n",
    "X_train_fe['FinishedBsmtRatio'] = X_train_fe['BsmtFinSF1'] / (X_train_fe['TotalBsmtSF'] + 1)\n",
    "X_test_fe['FinishedBsmtRatio'] = X_test_fe['BsmtFinSF1'] / (X_test_fe['TotalBsmtSF'] + 1)\n",
    "\n",
    "# Overall House Score\n",
    "X_train_fe['OverallScore'] = X_train_fe['OverallQual'] * X_train_fe['OverallCond']\n",
    "X_test_fe['OverallScore'] = X_test_fe['OverallQual'] * X_test_fe['OverallCond']\n",
    "\n",
    "# Porch Existence\n",
    "X_train_fe['HasPorch'] = (X_train_fe['OpenPorchSF'] > 0).astype(int)\n",
    "X_test_fe['HasPorch'] = (X_test_fe['OpenPorchSF'] > 0).astype(int)\n",
    "\n",
    "# Sanity Check\n",
    "print(f\"[INFO] New features added: {X_train_fe.shape[1] - X_train_scaled.shape[1]}\")\n",
    "print(\"[INFO] Feature Engineering completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b6859",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "*HouseAge*- Older homes usually have different prices.\n",
    "\n",
    "*LivingLotRatio*- How efficiently the land is used.\n",
    "\n",
    "*FinishedBsmtRatio*- Bigger finished basements = higher value.\n",
    "\n",
    "*OverallScore*- Combines quality + condition into one metric.\n",
    "\n",
    "*HasPorch*- Small porches can make a big price difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751b0ee",
   "metadata": {},
   "source": [
    "### Validate New Features\n",
    "\n",
    "We must verify that the newly added features are appropriately included by looking at a preview of the revised datasets now that we have developed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate newly engineered features by plotting their distributions and boxplots.\n",
    "\n",
    "This visual check helps ensure:\n",
    "- Value ranges are reasonable (e.g., no outliers or NaNs)\n",
    "- Binary features like 'HasPorch' contain only 0 and 1\n",
    "- Skewed features reflect expected transformations\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create validation visuals output folder\n",
    "validation_output_dir = \"outputs/visuals/new_features\"\n",
    "os.makedirs(validation_output_dir, exist_ok=True)\n",
    "\n",
    "# List of new features to validate\n",
    "new_features = ['HouseAge', 'LivingLotRatio', 'FinishedBsmtRatio', 'OverallScore', 'HasPorch']\n",
    "\n",
    "# Plot distributions and boxplots\n",
    "for feature in new_features:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Distribution Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_train_fe[feature], kde=True, bins=30, color='skyblue')\n",
    "    plt.title(f\"{feature} - Distribution\")\n",
    "    \n",
    "    # Box Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=X_train_fe[feature], color='lightcoral')\n",
    "    plt.title(f\"{feature} - Boxplot\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{validation_output_dir}/{feature}_validation.png\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"[INFO] New feature validation plots saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219404e5",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "*HouseAge*- Should be mostly positive, reasonable range (like 0-200 years).\n",
    "\n",
    "*LivingLotRatio*- Should cluster below 1. High values = cramped properties.\n",
    "\n",
    "*FinishedBsmtRatio*- Should mostly stay between 0 and 1.\n",
    "\n",
    "*OverallScore*- Should be spread depending on quality and condition variations.\n",
    "\n",
    "*HasPorch*- Binary — only 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4af1c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Feature Engineering phase plays a pivotal role in transforming raw data into meaningful signals for machine learning algorithms. Here's what we accomplished in this notebook:\n",
    "\n",
    "### Targeted Transformations\n",
    "- Applied logarithmic transformations on positively skewed variables (e.g., LotArea, GrLivArea) to normalize distributions, making them more suitable for linear models.\n",
    "- Applied reflection techniques for negatively skewed features (e.g., YearBuilt, BedroomAbvGr) to counteract distributional imbalance.\n",
    "- Validated each transformation visually using histograms and KDE plots, and exported them for documentation.\n",
    "\n",
    "### Scaling & Encoding\n",
    "- StandardScaler was used to normalize numerical values, ensuring each feature contributes equally to model training.\n",
    "- Applied One-Hot Encoding to categorical variables, preserving key categorical distinctions in a machine-readable format.\n",
    "\n",
    "### Multicollinearity Handling\n",
    "- Performed correlation analysis and heatmap visualisation to detect multicollinearity.\n",
    "- Computed Variance Inflation Factor (VIF) for numerical features.\n",
    "- Identified and removed highly collinear variables (VIF > 10), reducing redundancy and improving model stability.\n",
    "\n",
    "### New Feature Creation\n",
    "- Engineered HouseAge, LivingLotRatio, FinishedBsmtRatio, OverallScore, and HasPorch — derived variables that better capture the underlying structural and design-related drivers of house prices.\n",
    "- Verified data integrity post-creation, ensuring no nulls or erroneous values were introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f615b",
   "metadata": {},
   "source": [
    "## Creating a Reusable Preprocessing Pipeline\n",
    "\n",
    "To ensure consistent data transformation between training and deployment (e.g., in the Streamlit app), we construct a unified preprocessing pipeline using ColumnTransformer. This includes:\n",
    "\n",
    "- Scaling of numerical features\n",
    "- One-hot encoding of categorical features\n",
    "\n",
    "The final pipeline will be saved as preprocessor.pkl and used during prediction to transform new input data the same way our training data was processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e99e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Final output directory\n",
    "output_dir = \"data/processed/final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save transformed features and target\n",
    "X_train_fe.to_csv(f\"{output_dir}/X_train.csv\", index=False)\n",
    "X_test_fe.to_csv(f\"{output_dir}/X_test.csv\", index=False)\n",
    "pd.Series(y_train).to_csv(f\"{output_dir}/y_train.csv\", index=False)\n",
    "pd.Series(y_test).to_csv(f\"{output_dir}/y_test.csv\", index=False)\n",
    "\n",
    "print(f\"[SAVED] Final engineered datasets saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63900d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With the feature engineering pipeline complete and validated, we’re ready to progress toward model development. Here’s the roadmap:\n",
    "\n",
    "### Model Development\n",
    "- Baseline Modeling\n",
    "- Train a baseline model (e.g., Linear Regression) to establish benchmark metrics (MAE, RMSE, R2).\n",
    "- Advanced Algorithms\n",
    "- Train tree-based models (Random Forest, Gradient Boosting, XGBoost) to capture nonlinear interactions.\n",
    "\n",
    "### Model Evaluation\n",
    "- Use consistent evaluation metrics across models.\n",
    "- Plot residuals, prediction error distributions, and learning curves.\n",
    "\n",
    "### Model Optimization\n",
    "- Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV.\n",
    "- Cross-validate performance on training data using K-Fold CV.\n",
    "\n",
    "### Model Selection & Interpretation\n",
    "- Select the best-performing model based on generalization performance.\n",
    "- Investigate feature importance using permutation, SHAP, or built-in model attributes.\n",
    "\n",
    "### Deployment Prep \n",
    "- Package final model and transformations into a pipeline.\n",
    "- Create a simple Streamlit or Flask app for interactive predictions.\n",
    "- Push to GitHub and document steps for reproducibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
