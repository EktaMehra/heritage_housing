{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a755667",
   "metadata": {},
   "source": [
    "_This project was developed independently as part of Code Instituteâ€™s Predictive Analytics Project. Any datasets or templates used are openly provided by the course or via public sources like Kaggle. All commentary and code logic are my own._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db563f",
   "metadata": {},
   "source": [
    "# 06 Final Pipeline And Deployment Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3248b9",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "This notebook consolidates the final machine learning pipeline for the Heritage Housing project. The primary goals are:\n",
    "\n",
    "- Load model components and training artefacts\n",
    "- Rebuild and validate the final predictive pipeline\n",
    "- Apply predictions to test and new data\n",
    "- Serialize and prepare the pipeline for deployment\n",
    "- Ensure full compatibility with app integration and dashboard outputs\n",
    "\n",
    "### Inputs  \n",
    "- All trained model files from `/outputs/models/`  \n",
    "- Evaluation and SHAP files from `/outputs/`  \n",
    "- `inherited_houses.csv` (raw data for inference)\n",
    "\n",
    "### Outputs  \n",
    "- Final serialized pipeline: `/outputs/models/final_random_forest_pipeline.pkl`  \n",
    "- Inherited predictions:  \n",
    "  - `/data/processed/final/inherited_properties_display_ready.csv`  \n",
    "  - `/outputs/predictions/inherited_price_forecast.csv`  \n",
    "- Visuals and residual plots: `/outputs/visuals/`  \n",
    "- Dashboard-ready CSVs and validation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e1dc04",
   "metadata": {},
   "source": [
    "## Change Working Directory\n",
    "- Since it is expected that you would keep the notebooks in a subfolder, you will need to switch the working directory when you run the notebook in the editor.\n",
    "- The working directory must be changed from its current folder to its parent folder.\n",
    "- We wish to change the current directory's parent to the new current directory.\n",
    "- Verify the updated current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to smart working directory\n",
    "import os\n",
    "\n",
    "project_root = '/workspaces/heritage_housing'\n",
    "if os.getcwd() != project_root:\n",
    "    try:\n",
    "        os.chdir(project_root)\n",
    "        print(f\"[INFO] Working directory changed to: {os.getcwd()}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"[ERROR] Project root '{project_root}' not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc4797",
   "metadata": {},
   "source": [
    "## Requirements: Import Libraries, Verify Dependencies, Load Artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d652698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Verify Dependencies (Optional Version Check)\n",
    "\n",
    "required_dependencies = {\n",
    "    \"pandas\": \"2.1.1\",\n",
    "    \"numpy\": \"1.26.1\",\n",
    "    \"matplotlib\": \"3.8.0\",\n",
    "    \"seaborn\": \"0.13.2\",\n",
    "    \"joblib\": \"1.4.2\"\n",
    "}\n",
    "\n",
    "installed_dependencies = {}\n",
    "for lib, expected_version in required_dependencies.items():\n",
    "    try:\n",
    "        lib_version = __import__(lib).__version__\n",
    "        installed_dependencies[lib] = lib_version\n",
    "        if lib_version != expected_version:\n",
    "            print(f\"[WARNING] {lib} version mismatch: expected {expected_version}, found {lib_version}\")\n",
    "        else:\n",
    "            print(f\"[OK] {lib} version: {lib_version}\")\n",
    "    except ImportError:\n",
    "        print(f\"[MISSING] {lib} is not installed!\")\n",
    "\n",
    "print(\"\\nInstalled Dependencies Summary:\")\n",
    "print(json.dumps(installed_dependencies, indent=4))\n",
    "\n",
    "# Define Artefact Paths\n",
    "artifacts_paths = {\n",
    "    \"best_rf_model\": \"outputs/models/best_random_forest.pkl\",\n",
    "    \"best_dt_model\": \"outputs/models/best_decision_tree.pkl\",\n",
    "    \"best_gbr_model\": \"outputs/models/best_gradient_boosting.pkl\",\n",
    "    \"best_ridge_model\": \"outputs/models/best_ridge.pkl\",\n",
    "    \"best_svr_model\": \"outputs/models/best_svr.pkl\",\n",
    "    \"evaluation_metrics\": \"outputs/metrics/consolidated_model_performance.csv\",\n",
    "    \"cv_results\": \"outputs/metrics/cross_validation_results.csv\",\n",
    "    \"test_set_results\": \"outputs/metrics/test_set_results.csv\",\n",
    "    \"feature_importance_rf\": \"outputs/ft_importance/random_forest_importance.csv\",\n",
    "    \"feature_importance_gbr\": \"outputs/ft_importance/xgboost_importance.csv\",\n",
    "    \"shap_rf\": \"outputs/shap_values/shap_summary_random_forest.csv\",\n",
    "    \"shap_gbr\": \"outputs/shap_values/shap_summary_xgboost.csv\",\n",
    "    \"X_train\": \"data/processed/final/X_train.csv\",\n",
    "    \"X_test\": \"data/processed/final/X_test.csv\",\n",
    "    \"y_train\": \"data/processed/final/y_train.csv\",\n",
    "    \"y_test\": \"data/processed/final/y_test.csv\",\n",
    "}\n",
    "\n",
    "# Load Saved Models\n",
    "models = {}\n",
    "for key in [\"best_rf_model\", \"best_dt_model\", \"best_gbr_model\", \"best_ridge_model\", \"best_svr_model\"]:\n",
    "    try:\n",
    "        models[key] = joblib.load(artifacts_paths[key])\n",
    "        print(f\"[LOADED] {key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not load {key}: {e}\")\n",
    "\n",
    "# Load Metrics and SHAP Results\n",
    "evaluation_metrics = pd.read_csv(artifacts_paths[\"evaluation_metrics\"])\n",
    "cv_results = pd.read_csv(artifacts_paths[\"cv_results\"])\n",
    "test_set_results = pd.read_csv(artifacts_paths[\"test_set_results\"])\n",
    "feature_importance_rf = pd.read_csv(artifacts_paths[\"feature_importance_rf\"])\n",
    "feature_importance_gbr = pd.read_csv(artifacts_paths[\"feature_importance_gbr\"])\n",
    "shap_rf = pd.read_csv(artifacts_paths[\"shap_rf\"])\n",
    "shap_gbr = pd.read_csv(artifacts_paths[\"shap_gbr\"])\n",
    "\n",
    "# Load Train/Test Splits\n",
    "X_train = pd.read_csv(artifacts_paths[\"X_train\"])\n",
    "X_test = pd.read_csv(artifacts_paths[\"X_test\"])\n",
    "y_train = pd.read_csv(artifacts_paths[\"y_train\"]).values.ravel()\n",
    "y_test = pd.read_csv(artifacts_paths[\"y_test\"]).values.ravel()\n",
    "\n",
    "print(\"[INFO] All data and artefacts loaded successfully.\")\n",
    "\n",
    "X_train = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "print(\"[DEBUG] X_train sample columns:\")\n",
    "print(X_train.columns.tolist()[:20])\n",
    "\n",
    "# Preview Key Artefacts\n",
    "print(\"\\n[PREVIEW] Evaluation Metrics:\")\n",
    "display(evaluation_metrics.head())\n",
    "\n",
    "print(\"\\n[PREVIEW] Cross Validation Results:\")\n",
    "display(cv_results.head())\n",
    "\n",
    "print(\"\\n[PREVIEW] Test Set Results:\")\n",
    "display(test_set_results.head())\n",
    "\n",
    "print(\"\\n[PREVIEW] Feature Importance (Random Forest):\")\n",
    "display(feature_importance_rf.head())\n",
    "\n",
    "print(\"\\n[PREVIEW] Feature Importance (XGBoost):\")\n",
    "display(feature_importance_gbr.head())\n",
    "\n",
    "print(\"\\n[PREVIEW] SHAP Summary (Random Forest):\")\n",
    "display(shap_rf.head())\n",
    "\n",
    "print(\"\\n[PREVIEW] SHAP Summary (XGBoost):\")\n",
    "display(shap_gbr.head())\n",
    "\n",
    "X_train = pd.read_csv(artifacts_paths[\"X_train\"])\n",
    "# Preview encoded categorical columns to check casing\n",
    "X_train.columns[X_train.columns.str.contains(\"cat__\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d354f66",
   "metadata": {},
   "source": [
    "## Pipeline Design\n",
    "\n",
    "This section defines the preprocessing and modeling pipeline. It includes:\n",
    "- Feature scaling for numerical columns\n",
    "- Encoding for categorical columns\n",
    "- Integration with the best-performing model (Random Forest)\n",
    "- Preparation of a final deployable `Pipeline` object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final Preprocessing Pipeline\n",
    "\n",
    "This script defines a numerical preprocessing pipeline using `Pipeline` and `ColumnTransformer`.\n",
    "It includes:\n",
    "- Median imputation for missing numeric values\n",
    "- Standard scaling for normalizing feature ranges\n",
    "\n",
    "This pipeline will be serialized and integrated with the trained model for consistent preprocessing\n",
    "during deployment and inference.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Detect numerical features\n",
    "numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"[INFO] Numerical features detected: {len(numerical_cols)}\")\n",
    "\n",
    "# Define numeric preprocessing steps\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),   # Fill missing values with median\n",
    "    (\"scaler\", StandardScaler())                     # Normalize features to 0 mean, 1 std\n",
    "])\n",
    "\n",
    "# Create full ColumnTransformer with numerical pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numerical_pipeline, numerical_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db9a4a",
   "metadata": {},
   "source": [
    "### Model Integration and Prediction Comparison\n",
    "\n",
    "We now evaluate the top two models (Random Forest and XGBoost) on test data and compute MAE for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def integrate_model(model, features, model_name):\n",
    "    \"\"\"\n",
    "    Apply a trained model to features and return prediction DataFrame with model name.\n",
    "    \"\"\"\n",
    "    preds = model.predict(features)\n",
    "    return pd.DataFrame({\n",
    "        \"Model\": model_name,\n",
    "        \"Predicted_LogSalePrice\": preds\n",
    "    })\n",
    "\n",
    "# Generate predictions\n",
    "rf_preds_df = integrate_model(models[\"best_rf_model\"], X_test, \"Random Forest\")\n",
    "gbr_preds_df = integrate_model(models[\"best_gbr_model\"], X_test, \"XGBoost\")\n",
    "\n",
    "# Combine and append actual values\n",
    "combined_preds_df = pd.concat([rf_preds_df, gbr_preds_df], ignore_index=True).copy()\n",
    "combined_preds_df[\"Actual_LogSalePrice\"] = list(y_test) * 2\n",
    "combined_preds_df[\"Predicted_Price\"] = np.expm1(combined_preds_df[\"Predicted_LogSalePrice\"])\n",
    "combined_preds_df[\"Actual_Price\"] = np.expm1(combined_preds_df[\"Actual_LogSalePrice\"])\n",
    "\n",
    "display(combined_preds_df.head())\n",
    "\n",
    "# Evaluate and save comparison\n",
    "mae_by_model = combined_preds_df.groupby(\"Model\").apply(\n",
    "    lambda df: mean_absolute_error(df[\"Actual_LogSalePrice\"], df[\"Predicted_LogSalePrice\"])\n",
    ").reset_index(name=\"MAE_LogPrice\")\n",
    "\n",
    "print(\"[RESULT] MAE (Log Sale Price) by Model:\")\n",
    "display(mae_by_model)\n",
    "\n",
    "# Save metrics\n",
    "output_path = \"outputs/metrics/mae_by_model.csv\"\n",
    "mae_by_model.to_csv(output_path, index=False)\n",
    "print(f\"[SAVED] MAE by model saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870c55d",
   "metadata": {},
   "source": [
    "### Final Pipeline Assembly\n",
    "\n",
    "We now fit the `preprocessor` on training data and combine it with the best model into a final deployment pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Assemble final pipeline with preprocessor + best RF model\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", models[\"best_rf_model\"])  # Loaded from artifacts_paths\n",
    "])\n",
    "\n",
    "# Save final pipeline\n",
    "pipeline_path = \"outputs/models/final_random_forest_pipeline.pkl\"\n",
    "os.makedirs(os.path.dirname(pipeline_path), exist_ok=True)\n",
    "joblib.dump(final_pipeline, pipeline_path)\n",
    "\n",
    "print(f\"[SAVED] Final pipeline stored at: {pipeline_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34380d81",
   "metadata": {},
   "source": [
    "## Prediction Pipeline\n",
    "\n",
    "This section demonstrates how to apply the final trained models to new data â€” specifically, inherited properties. It includes preprocessing, feature alignment, prediction, and generating client-ready output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac57538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_properties(raw_df, training_features, processed_save_path=None):\n",
    "    \"\"\"\n",
    "    Preprocess raw property data to align with training features.\n",
    "    Assumes X_train-style column names (no prefixing).\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Preprocessing raw data for prediction...\")\n",
    "\n",
    "    # One-hot encode\n",
    "    cat_cols = raw_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    raw_df = pd.get_dummies(raw_df, columns=cat_cols, drop_first=True)\n",
    "    print(f\"[INFO] One-hot encoded {len(cat_cols)} categorical columns.\")\n",
    "\n",
    "    # Feature Engineering\n",
    "    required_cols = ['YearBuilt', 'GrLivArea', 'LotArea', 'BsmtFinSF1', 'TotalBsmtSF', 'OverallQual', 'OverallCond']\n",
    "    if all(col in raw_df.columns for col in required_cols):\n",
    "        raw_df[\"HouseAge\"] = 2025 - raw_df[\"YearBuilt\"]\n",
    "        raw_df[\"LivingLotRatio\"] = raw_df[\"GrLivArea\"] / (raw_df[\"LotArea\"] + 1)\n",
    "        raw_df[\"FinishedBsmtRatio\"] = raw_df[\"BsmtFinSF1\"] / (raw_df[\"TotalBsmtSF\"] + 1)\n",
    "        raw_df[\"OverallScore\"] = raw_df[\"OverallQual\"] * raw_df[\"OverallCond\"]\n",
    "        raw_df.drop(columns=required_cols, inplace=True)\n",
    "        print(\"[INFO] Feature engineering complete.\")\n",
    "    else:\n",
    "        print(\"[WARNING] Not all required columns found for feature engineering.\")\n",
    "\n",
    "    # Debug before alignment\n",
    "    print(\"[DEBUG] Processed raw_df columns:\", raw_df.columns.tolist())\n",
    "    print(\"[DEBUG] Expected X_train columns:\", training_features.columns.tolist())\n",
    "    print(\"[DEBUG] Overlap:\", len(set(raw_df.columns) & set(training_features.columns)))\n",
    "\n",
    "    # Align with training columns\n",
    "    for col in training_features.columns:\n",
    "        if col not in raw_df.columns:\n",
    "            raw_df[col] = 0\n",
    "    raw_df = raw_df[training_features.columns]\n",
    "\n",
    "    print(\"[INFO] Columns aligned with training data.\")\n",
    "\n",
    "    if processed_save_path:\n",
    "        os.makedirs(os.path.dirname(processed_save_path), exist_ok=True)\n",
    "        raw_df.to_csv(processed_save_path, index=False)\n",
    "        print(f\"[SAVED] Processed data to: {processed_save_path}\")\n",
    "\n",
    "    return raw_df\n",
    "\n",
    "inherited_raw_df = pd.read_csv(\"data/raw/inherited_houses.csv\")\n",
    "X_train = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "\n",
    "processed_inherited = process_new_properties(\n",
    "    raw_df=inherited_raw_df,\n",
    "    training_features=X_train,\n",
    "    processed_save_path=\"data/processed/final/inherited_processed.csv\"\n",
    ")\n",
    "\n",
    "print(\"[CHECK] Processed shape:\", processed_inherited.shape)\n",
    "print(\"[CHECK] Sample columns:\", processed_inherited.columns.tolist()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff95cd",
   "metadata": {},
   "source": [
    "## Prediction Pipeline: Inherited Properties Forecast\n",
    "\n",
    "We now apply the trained models to the processed inherited properties dataset. This step simulates post-deployment usage where new raw data is processed and predictions are generated.\n",
    "\n",
    "Key steps included:\n",
    "\n",
    "- Applying feature engineering and column alignment to raw inherited data\n",
    "- Predicting LogSalePrice using both Random Forest and XGBoost\n",
    "- Merging predictions into a combined DataFrame for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed input for prediction\n",
    "inherited_properties_processed = pd.read_csv(\"data/processed/final/inherited_processed.csv\")\n",
    "\n",
    "# Predict using Random Forest\n",
    "rf_inherited_predictions = integrate_model(\n",
    "    models[\"best_rf_model\"], inherited_properties_processed, \"Random Forest\"\n",
    ")\n",
    "rf_inherited_predictions[\"Predicted SalePrice\"] = np.expm1(rf_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "print(\"[INFO] RF Inherited Predictions:\")\n",
    "display(rf_inherited_predictions)\n",
    "\n",
    "# Predict using XGBoost\n",
    "gbr_inherited_predictions = integrate_model(\n",
    "    models[\"best_gbr_model\"], inherited_properties_processed, \"XGBoost\"\n",
    ")\n",
    "gbr_inherited_predictions[\"Predicted SalePrice\"] = np.expm1(gbr_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "print(\"[INFO] XGB Inherited Predictions:\")\n",
    "display(gbr_inherited_predictions)\n",
    "\n",
    "# Combine predictions into single DataFrame\n",
    "inherited_combined_predictions = pd.concat([\n",
    "    rf_inherited_predictions,\n",
    "    gbr_inherited_predictions\n",
    "], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(\"[INFO] Combined Model Predictions:\")\n",
    "display(inherited_combined_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efbc36",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- The Random Forest and XGBoost models produced consistent LogSalePrice estimates.\n",
    "- Variation between models reflects differing feature weightings and split strategies.\n",
    "- Predictions fall within expected bands based on prior analysis.\n",
    "\n",
    "This shows the pipeline is operating as expected when applied to new, unseen data.\n",
    "\n",
    "### Save Display-Ready Dataset for Dashboard\n",
    "\n",
    "Now we'll create a dashboard-friendly dataset that:\n",
    "- Combines original inherited house features with predicted prices\n",
    "- Is saved to CSV for Streamlit or client delivery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78bfe5",
   "metadata": {},
   "source": [
    "## Save Display-Ready Dataset for Dashboard\n",
    "\n",
    "The prediction input was optimized for machine learning (scaled, encoded, engineered). However, for client use, we need a version that:\n",
    "\n",
    "- Retains raw, human-readable attributes\n",
    "- Shows predicted LogSalePrice and actual predicted SalePrice\n",
    "- Labels the model used\n",
    "\n",
    "This dataset is suitable for dashboards, reports, or presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload raw inherited property data\n",
    "inherited_raw_df = pd.read_csv(\"data/raw/inherited_houses.csv\")\n",
    "\n",
    "# Reset predictions\n",
    "rf_preds = rf_inherited_predictions.reset_index(drop=True)\n",
    "gbr_preds = gbr_inherited_predictions.reset_index(drop=True)\n",
    "\n",
    "# Create display-friendly versions\n",
    "rf_display = inherited_raw_df.copy()\n",
    "gbr_display = inherited_raw_df.copy()\n",
    "\n",
    "rf_display[\"Model\"] = \"Random Forest\"\n",
    "gbr_display[\"Model\"] = \"XGBoost\"\n",
    "\n",
    "rf_display[\"Predicted_LogSalePrice\"] = rf_preds[\"Predicted_LogSalePrice\"]\n",
    "rf_display[\"Predicted_SalePrice\"] = rf_preds[\"Predicted SalePrice\"]\n",
    "\n",
    "gbr_display[\"Predicted_LogSalePrice\"] = gbr_preds[\"Predicted_LogSalePrice\"]\n",
    "gbr_display[\"Predicted_SalePrice\"] = gbr_preds[\"Predicted SalePrice\"]\n",
    "\n",
    "# Combine and save\n",
    "dashboard_ready_df = pd.concat([rf_display, gbr_display], axis=0)\n",
    "\n",
    "dashboard_output_path = \"outputs/predictions/inherited_display_ready.csv\"\n",
    "os.makedirs(os.path.dirname(dashboard_output_path), exist_ok=True)\n",
    "dashboard_ready_df.to_csv(dashboard_output_path, index=False)\n",
    "\n",
    "print(f\"[SAVED] Dashboard-ready file saved to: {dashboard_output_path}\")\n",
    "display(dashboard_ready_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e1e7a",
   "metadata": {},
   "source": [
    "In this section, we finalized the creation of a display-ready dataset based on the processed inherited properties dataset. The dataset:\n",
    "\n",
    "- Retains all original (raw, human-readable) property features.\n",
    "- Integrates predicted sale prices from both the Random Forest and XGBoost models.\n",
    "- Combines predictions into a client-readable format with clearly labeled columns.\n",
    "- Ensures compatibility for dashboard integration and client presentation.\n",
    "\n",
    "The resulting dataset has been saved to:\n",
    "`outputs/predictions/inherited_display_ready.csv`\n",
    "\n",
    "It is now ready for use in visualization, comparison, and business insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41b52d",
   "metadata": {},
   "source": [
    "**Save Display-Ready Dataset for Dashboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc746fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new inherited property dataset\n",
    "inherited_df = pd.read_csv(\"data/raw/inherited_houses.csv\")\n",
    "\n",
    "# Process for prediction\n",
    "processed_inherited = process_new_properties(\n",
    "    raw_df=inherited_df,\n",
    "    training_features=X_train,\n",
    "    processed_save_path=\"data/processed/final/inherited_processed.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87fec0",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "This section validates the model pipeline using:\n",
    "- The official test set (with known targets)\n",
    "- The processed inherited properties (with simulated or qualitative validation)\n",
    "\n",
    "The test predictions are evaluated using:\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- RÂ² Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b38de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Validation â€“ Test Set Performance\n",
    "\n",
    "This script compares the performance of trained models (Random Forest and XGBoost/Gradient Boosting)\n",
    "on the test dataset. It outputs:\n",
    "\n",
    "- Predictions vs actuals\n",
    "- Residual errors\n",
    "- Evaluation metrics (R2, MAE, RMSE)\n",
    "- Scatterplot for visual comparison\n",
    "- CSV export of final test metrics\n",
    "\"\"\"\n",
    "\n",
    "# Imports \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Output directories \n",
    "os.makedirs(\"outputs/visuals\", exist_ok=True)\n",
    "os.makedirs(\"outputs/metrics\", exist_ok=True)\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions and return results and metrics.\n",
    "\n",
    "    Parameters:\n",
    "        model: Fitted regression model\n",
    "        X_test: Test features\n",
    "        y_test: True target values (log-transformed)\n",
    "        model_name: Name of the model (str)\n",
    "\n",
    "    Returns:\n",
    "        results: DataFrame with actual, predicted, and residual values\n",
    "        metrics: Dictionary with R2, MAE, and RMSE\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        \"Model\": model_name,\n",
    "        \"Actual\": y_test,\n",
    "        \"Predicted\": y_pred,\n",
    "        \"Residual\": residuals\n",
    "    })\n",
    "\n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"R2\": r2_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    return results, metrics\n",
    "\n",
    "# Evaluate Models\n",
    "rf_results_df, rf_metrics = validate_model(models[\"best_rf_model\"], X_test, y_test, \"Random Forest\")\n",
    "gbr_results_df, gbr_metrics = validate_model(models[\"best_gbr_model\"], X_test, y_test, \"Gradient Boosting\")\n",
    "\n",
    "# Show Sample Predictions\n",
    "print(\"Random Forest â€“ First 10 Predictions\")\n",
    "display(rf_results_df.head(10))\n",
    "\n",
    "print(\"Gradient Boosting â€“ First 5 Predictions\")\n",
    "display(gbr_results_df.head(5))\n",
    "\n",
    "# Plot Predictions vs Actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=rf_results_df[\"Actual\"], y=rf_results_df[\"Predicted\"], label=\"Random Forest\", alpha=0.7)\n",
    "sns.scatterplot(x=gbr_results_df[\"Actual\"], y=gbr_results_df[\"Predicted\"], label=\"Gradient Boosting\", alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual LogSalePrice\")\n",
    "plt.ylabel(\"Predicted LogSalePrice\")\n",
    "plt.title(\"Predicted vs Actual â€“ Random Forest vs Gradient Boosting\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Plot\n",
    "plot_path = \"outputs/visuals/predicted_vs_actual_rf_vs_gbr.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "print(f\"[SAVED] Validation plot saved to: {plot_path}\")\n",
    "\n",
    "# Save Evaluation Metrics\n",
    "model_metrics_df = pd.DataFrame([rf_metrics, gbr_metrics])\n",
    "print(\"[RESULT] Model Performance Summary:\")\n",
    "display(model_metrics_df)\n",
    "\n",
    "metrics_path = \"outputs/metrics/test_validation_metrics.csv\"\n",
    "model_metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"[SAVED] Test set metrics saved to: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f45df4",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "**Random Forest Model:**\n",
    "- MAE: \t0.101793\n",
    "- R2 Score: 0.876637\n",
    "- Residuals: Small, indicating high accuracy and minimal error.\n",
    "\n",
    "**XGBoost Model:**\n",
    "- MAE: \t0.099785\n",
    "- R2 Score: 0.877383\n",
    "- Residuals: Slightly larger, but still strong predictive power.\n",
    "\n",
    "### Comparative Analysis:\n",
    "The Random Forest model slightly outperforms XGBoost in both MAE and R2, indicating it provides slightly more reliable predictions on unseen data. However, both models generalize well and show strong performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d27b9",
   "metadata": {},
   "source": [
    "**Hypothetical Validation for Inherited Properties**\n",
    "\n",
    "Since we donâ€™t have actual sale prices for the inherited properties, weâ€™ll simulate hypothetical values only for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86281cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hypothetical Evaluation â€“ Inherited Properties\n",
    "\n",
    "This script simulates ground-truth values for inherited homes and compares them\n",
    "to predicted values from both Random Forest and XGBoost models. \n",
    "\n",
    "It calculates residual errors, MAE, and R2, and visualizes the model accuracy\n",
    "on hypothetical (unseen) property valuations.\n",
    "\n",
    "Assumptions:\n",
    "- The true sale prices (log-transformed) for the 4 inherited properties are approximated.\n",
    "\"\"\"\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# --- Create output directory ---\n",
    "os.makedirs(\"outputs/visuals\", exist_ok=True)\n",
    "\n",
    "# --- Simulated Ground Truths ---\n",
    "# These values are assumed for demonstration â€“ not real data\n",
    "hypothetical_actuals = [11.70, 12.30, 11.95, 12.10]\n",
    "assert len(hypothetical_actuals) == len(rf_inherited_predictions), \"Mismatch in predicted vs actual count.\"\n",
    "\n",
    "# --- Add to DataFrames ---\n",
    "rf_inherited_predictions[\"Hypothetical_Actual\"] = hypothetical_actuals\n",
    "gbr_inherited_predictions[\"Hypothetical_Actual\"] = hypothetical_actuals\n",
    "\n",
    "# Calculate residuals\n",
    "rf_inherited_predictions[\"Residual\"] = rf_inherited_predictions[\"Hypothetical_Actual\"] - rf_inherited_predictions[\"Predicted_LogSalePrice\"]\n",
    "gbr_inherited_predictions[\"Residual\"] = gbr_inherited_predictions[\"Hypothetical_Actual\"] - gbr_inherited_predictions[\"Predicted_LogSalePrice\"]\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "rf_mae_hyp = mean_absolute_error(rf_inherited_predictions[\"Hypothetical_Actual\"], rf_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "rf_r2_hyp = r2_score(rf_inherited_predictions[\"Hypothetical_Actual\"], rf_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "\n",
    "gbr_mae_hyp = mean_absolute_error(gbr_inherited_predictions[\"Hypothetical_Actual\"], gbr_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "gbr_r2_hyp = r2_score(gbr_inherited_predictions[\"Hypothetical_Actual\"], gbr_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "\n",
    "# --- Print Summary ---\n",
    "print(f\"Random Forest â€“ MAE: {rf_mae_hyp:.4f}, R2: {rf_r2_hyp:.4f}\")\n",
    "print(f\"Gradient Boosting â€“ MAE: {gbr_mae_hyp:.4f}, R2: {gbr_r2_hyp:.4f}\")\n",
    "\n",
    "# --- Display Residuals ---\n",
    "print(\"Residuals â€“ Random Forest:\")\n",
    "display(rf_inherited_predictions[[\"Predicted_LogSalePrice\", \"Hypothetical_Actual\", \"Residual\"]])\n",
    "\n",
    "print(\"Residuals â€“ Gradient Boosting:\")\n",
    "display(gbr_inherited_predictions[[\"Predicted_LogSalePrice\", \"Hypothetical_Actual\", \"Residual\"]])\n",
    "\n",
    "# --- Plot Predictions vs Simulated Actuals ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=rf_inherited_predictions[\"Hypothetical_Actual\"],\n",
    "                y=rf_inherited_predictions[\"Predicted_LogSalePrice\"], label=\"Random Forest\", s=80)\n",
    "sns.scatterplot(x=gbr_inherited_predictions[\"Hypothetical_Actual\"],\n",
    "                y=gbr_inherited_predictions[\"Predicted_LogSalePrice\"], label=\"Gradient Boosting\", s=80)\n",
    "\n",
    "plt.plot([11.6, 12.4], [11.6, 12.4], 'r--')\n",
    "plt.xlabel(\"Hypothetical Actual LogSalePrice\")\n",
    "plt.ylabel(\"Predicted LogSalePrice\")\n",
    "plt.title(\"Model Predictions vs Hypothetical Actuals â€“ Inherited Properties\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- Save Plot ---\n",
    "plot_path = \"outputs/visuals/inherited_predictions_vs_hypothetical_actuals.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "print(f\"[SAVED] Validation plot saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928fd6e",
   "metadata": {},
   "source": [
    "### Validation: Hypothetical Evaluation for Inherited Properties\n",
    "\n",
    "Because we don't have actual sale prices for the inherited dataset, this section uses hypothetical values to simulate a post-deployment validation workflow.\n",
    "\n",
    "#### Random Forest:\n",
    "- MAE: **0.1869**\n",
    "- RÂ² Score: **-0.1697**\n",
    "- Residuals range: approx **-0.15 to +0.41**\n",
    "\n",
    "#### XGBoost:\n",
    "- MAE: **0.1570**\n",
    "- RÂ² Score: **0.2738**\n",
    "- Residuals range: approx **-0.16 to +0.31**\n",
    "\n",
    "#### Key Insights:\n",
    "- **Random Forest shows slightly higher variance** in prediction errors, especially on extreme values.\n",
    "- **XGBoost delivers a better RÂ²**, suggesting it captures relationships more effectively for this small test set.\n",
    "- Both models demonstrate **reasonable residuals** and simulate how they may perform once real sale prices are known.\n",
    "- This validation demonstrates the structure of post-deployment testing using feedback or future sale records.\n",
    "\n",
    "> This is a demonstration-only validation workflow designed for academic purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1626e",
   "metadata": {},
   "source": [
    "## Output Verification\n",
    "\n",
    "This section validates all key outputs from the pipeline, including:\n",
    "\n",
    "- Structure and completeness of prediction results\n",
    "- Accuracy of evaluation metrics\n",
    "- Residual distribution visualizations\n",
    "- Existence and integrity of the dashboard-ready dataset\n",
    "\n",
    "The goal is to ensure that every artefact is correctly formed and ready for deployment or presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58beef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final Output Verification â€“ Residual Metrics and Distribution Plots\n",
    "\n",
    "This script:\n",
    "- Validates prediction output formats for inherited property predictions\n",
    "- Computes residual error metrics (MAE, R2, min/max residuals)\n",
    "- Saves a summary CSV of model errors\n",
    "- Plots residual histograms and boxplots for each model\n",
    "\n",
    "These final validation steps assess how well each model performs under simulated test conditions and\n",
    "ensure the predictions are ready for interpretability and client display.\n",
    "\n",
    "Assumptions:\n",
    "- The columns 'Predicted_LogSalePrice' and 'Hypothetical_Actual' exist\n",
    "- Residuals have already been computed and added to each dataframe\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Setup folders\n",
    "os.makedirs(\"outputs/visuals\", exist_ok=True)\n",
    "os.makedirs(\"outputs/metrics\", exist_ok=True)\n",
    "\n",
    "# Check prediction result formats\n",
    "expected_columns = [\"Model\", \"Predicted_LogSalePrice\", \"Predicted SalePrice\"]\n",
    "print(\"[CHECK] Prediction Format Consistency\")\n",
    "\n",
    "for df, name in [(rf_inherited_predictions, \"Random Forest\"), (gbr_inherited_predictions, \"XGBoost\")]:\n",
    "    if all(col in df.columns for col in expected_columns):\n",
    "        print(f\"{name}: Format OK\")\n",
    "    else:\n",
    "        print(f\"{name}: Missing expected columns\")\n",
    "\n",
    "# Save residual metrics\n",
    "rf_mae_hyp = mean_absolute_error(rf_inherited_predictions[\"Hypothetical_Actual\"], rf_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "rf_r2_hyp = r2_score(rf_inherited_predictions[\"Hypothetical_Actual\"], rf_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "\n",
    "gbr_mae_hyp = mean_absolute_error(gbr_inherited_predictions[\"Hypothetical_Actual\"], gbr_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "gbr_r2_hyp = r2_score(gbr_inherited_predictions[\"Hypothetical_Actual\"], gbr_inherited_predictions[\"Predicted_LogSalePrice\"])\n",
    "\n",
    "residual_summary = pd.DataFrame({\n",
    "    \"Model\": [\"Random Forest\", \"XGBoost\"],\n",
    "    \"MAE\": [rf_mae_hyp, gbr_mae_hyp],\n",
    "    \"R2\": [rf_r2_hyp, gbr_r2_hyp],\n",
    "    \"Residual_Min\": [\n",
    "        rf_inherited_predictions[\"Residual\"].min(),\n",
    "        gbr_inherited_predictions[\"Residual\"].min()\n",
    "    ],\n",
    "    \"Residual_Max\": [\n",
    "        rf_inherited_predictions[\"Residual\"].max(),\n",
    "        gbr_inherited_predictions[\"Residual\"].max()\n",
    "    ]\n",
    "})\n",
    "\n",
    "metrics_path = \"outputs/metrics/hypothetical_residual_metrics.csv\"\n",
    "residual_summary.to_csv(metrics_path, index=False)\n",
    "print(f\"[SAVED] Residual metrics saved to: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a663d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot and save residual visuals\n",
    "def plot_residual_distribution(df, model_name, color):\n",
    "    \"\"\"\n",
    "    Plots histogram and boxplot of residuals for a given model.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'Residual' column.\n",
    "        model_name (str): Model name for labeling and saving.\n",
    "        color (str): Color for the plots.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df[\"Residual\"], kde=True, color=color, bins=20)\n",
    "    plt.title(f\"Residual Distribution â€“ {model_name}\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    file_path = f\"outputs/visuals/residual_distribution_{model_name.lower().replace(' ', '_')}.png\"\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "    print(f\"[SAVED] {model_name} histogram saved to: {file_path}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.boxplot(y=df[\"Residual\"], color=color)\n",
    "    plt.title(f\"Residual Boxplot â€“ {model_name}\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.tight_layout()\n",
    "    file_path = f\"outputs/visuals/residual_boxplot_{model_name.lower().replace(' ', '_')}.png\"\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "    print(f\"[SAVED] {model_name} boxplot saved to: {file_path}\")\n",
    "\n",
    "\n",
    "# Run residual analysis for both models\n",
    "print(\"\\n[ANALYSIS] Residuals â€“ Random Forest\")\n",
    "print(rf_inherited_predictions[\"Residual\"].describe())\n",
    "plot_residual_distribution(rf_inherited_predictions, \"Random Forest\", \"skyblue\")\n",
    "\n",
    "print(\"\\n[ANALYSIS] Residuals â€“ XGBoost\")\n",
    "print(gbr_inherited_predictions[\"Residual\"].describe())\n",
    "plot_residual_distribution(gbr_inherited_predictions, \"XGBoost\", \"orange\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53cdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for dashboard CSV\n",
    "display_path = \"outputs/predictions/inherited_display_ready.csv\"\n",
    "if os.path.exists(display_path):\n",
    "    print(f\"Display-ready CSV found: {display_path}\")\n",
    "    display(pd.read_csv(display_path).head())\n",
    "else:\n",
    "    print(\"Display-ready CSV not found. Please regenerate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9174274",
   "metadata": {},
   "source": [
    "### Output Verification Summary\n",
    "\n",
    "- **Prediction Format**: All required columns (`Model`, `Predicted_LogSalePrice`, `Predicted SalePrice`) are present\n",
    "- **Evaluation Metrics**: Residual MAE and RÂ² successfully calculated and saved for each model\n",
    "- **Residuals**: Distribution plots confirm residual spread and directionality\n",
    "- **Client Readability**: `inherited_display_ready.csv` is present, readable, and ready for dashboard integration\n",
    "\n",
    "All outputs have passed structural and content checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56f644",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "This section saves all final artefacts needed for deployment, dashboard integration, or API consumption.\n",
    "\n",
    "### Goals:\n",
    "- Save the final fitted pipeline (preprocessor + Random Forest model)\n",
    "- Confirm the presence and integrity of output artefacts\n",
    "- Copy dashboard-ready dataset to a stable location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final Model Pipeline Construction & Serialization\n",
    "\n",
    "This script:\n",
    "- Loads preprocessed training data\n",
    "- Builds a preprocessing + modeling pipeline using RandomForestRegressor\n",
    "- Ensures imputation, scaling, and model training are unified in a single pipeline\n",
    "- Serializes the fitted pipeline for deployment and reproducibility\n",
    "\n",
    "The pipeline will be used during inference (e.g., in Streamlit or other apps) to ensure \n",
    "new data goes through the same transformation steps as the training data.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load training data\n",
    "X_train = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/final/y_train.csv\").values.ravel()\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numerical_pipeline, numerical_cols)\n",
    "])\n",
    "\n",
    "# Build pipeline with model inside\n",
    "final_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(n_estimators=100, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the the pipeline on the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Load fitted model separately\n",
    "model = joblib.load(\"outputs/models/best_random_forest.pkl\")\n",
    "\n",
    "# Save the fitted pipeline\n",
    "pipeline_path = \"outputs/models/final_random_forest_pipeline.pkl\"\n",
    "os.makedirs(os.path.dirname(pipeline_path), exist_ok=True)\n",
    "joblib.dump(final_pipeline, pipeline_path)\n",
    "print(f\"[SAVED] Final pipeline saved to: {pipeline_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f06574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artefact Checklist\n",
    "artefacts_to_check = {\n",
    "    \"Final Pipeline Model\": pipeline_path,\n",
    "    \"Dashboard Dataset\": \"outputs/predictions/inherited_display_ready.csv\",\n",
    "    \"Test Validation Metrics\": \"outputs/metrics/test_validation_metrics.csv\",\n",
    "    \"Hypothetical Residuals\": \"outputs/metrics/hypothetical_residual_metrics.csv\"\n",
    "}\n",
    "\n",
    "# Verify each file\n",
    "print(\"\\n[CHECK] Artefact Verification:\")\n",
    "for label, path in artefacts_to_check.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"{label} found at: {path}\")\n",
    "    else:\n",
    "        print(f\"{label} missing at: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44905223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dashboard-ready CSV to a permanent location for deployment & add Filter to keep only Random Forest rows for display \n",
    "# Load prediction results\n",
    "df_display = pd.read_csv(\"outputs/predictions/inherited_display_ready.csv\")\n",
    "\n",
    "# Keep only rows where Model == \"Random Forest\"\n",
    "df_display = df_display[df_display[\"Model\"] == \"Random Forest\"].copy()\n",
    "\n",
    "# Assign Property_IDs explicitly\n",
    "df_display = df_display.reset_index(drop=True)\n",
    "df_display[\"Property_ID\"] = df_display.index\n",
    "\n",
    "# Save the corrected display-ready file\n",
    "final_display_path = \"data/processed/final/inherited_properties_display_ready.csv\"\n",
    "os.makedirs(os.path.dirname(final_display_path), exist_ok=True)\n",
    "df_display.to_csv(final_display_path, index=False)\n",
    "print(f\"[SAVED] Filtered display-ready dataset copied to: {final_display_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f9172",
   "metadata": {},
   "source": [
    "### Final Pipeline Structure Overview\n",
    "\n",
    "To ensure full transparency of the deployed model pipeline, we inspect the serialized `final_random_forest_pipeline.pkl` to confirm its internal structure and components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline\n",
    "pipeline_loaded = joblib.load(\"outputs/models/final_random_forest_pipeline.pkl\")\n",
    "\n",
    "# Show pipeline structure\n",
    "print(\"[INFO] Pipeline Steps:\")\n",
    "for name, step in pipeline_loaded.named_steps.items():\n",
    "    print(f\" - {name}: {type(step).__name__}\")\n",
    "\n",
    "# Show preprocessor and model separately\n",
    "preprocessor_loaded = pipeline_loaded.named_steps[\"preprocessor\"]\n",
    "model_loaded = pipeline_loaded.named_steps[\"model\"]\n",
    "\n",
    "print(f\"\\n[INFO] Preprocessor Type: {type(preprocessor_loaded).__name__}\")\n",
    "print(f\"[INFO] Model Type: {type(model_loaded).__name__}\")\n",
    "print(f\"[INFO] Number of input features expected by the model: {model_loaded.n_features_in_}\")\n",
    "\n",
    "# Get numeric feature names used during training\n",
    "if hasattr(model_loaded, \"feature_names_in_\"):\n",
    "    print(\"\\n[INFO] Feature names used by the model:\")\n",
    "    print(model_loaded.feature_names_in_.tolist())\n",
    "else:\n",
    "    print(\"\\n[INFO] Model does not store feature names directly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc09534",
   "metadata": {},
   "source": [
    "### Serialization Summary\n",
    "\n",
    "All final components are now saved and ready for deployment:\n",
    "\n",
    "- **Random Forest Model Pipeline**: `outputs/models/final_random_forest_pipeline.pkl`\n",
    "- **Dashboard Dataset**: `inherited_display_ready.csv`\n",
    "- **Evaluation Metrics**: `test_validation_metrics.csv`, `hypothetical_residual_metrics.csv`\n",
    "\n",
    "These artefacts are validated, version-compatible, and ready for integration into Streamlit, Flask, or external APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d9da2",
   "metadata": {},
   "source": [
    "## Deployment Preparation & Inference\n",
    "\n",
    "This section prepares the final model pipeline for real-world use with new, unseen data.\n",
    "\n",
    "### Features:\n",
    "- Reusable preprocessing for general input\n",
    "- Pipelines saved with both preprocessing + model steps\n",
    "- Example inference with new sample input\n",
    "- Output saved and validated\n",
    "\n",
    "### Inference Process:\n",
    "\n",
    "1. Load new raw data\n",
    "2. Preprocess the input using the same steps as training\n",
    "3. Load the saved `.pkl` pipeline\n",
    "4. Generate predictions\n",
    "5. Save output to CSV for client use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5155fa9",
   "metadata": {},
   "source": [
    "#### Clean Inference Script Format\n",
    "\n",
    "Weâ€™ll repackage your workflow as a single, reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deployment Inference Script â€“ Predicting Heritage House Prices\n",
    "\n",
    "This script:\n",
    "- Loads raw data for inherited heritage-listed properties\n",
    "- Applies preprocessing and engineered feature transformations to match training data\n",
    "- Uses the final serialized pipeline (preprocessor + trained Random Forest model) to generate predictions\n",
    "- Outputs both log-transformed and original scale price predictions\n",
    "- Saves the results as a CSV for display or further analysis\n",
    "\n",
    "Designed for use in a deployment/inference context where new housing records require price estimation.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- DEPLOYMENT PREPARATION & INFERENCE ---\n",
    "\n",
    "# Load raw input data\n",
    "inherited_raw_df = pd.read_csv(\"data/raw/inherited_houses.csv\")\n",
    "print(f\"[INFO] Loaded {inherited_raw_df.shape[0]} inherited properties.\")\n",
    "\n",
    "# Load reference training feature set\n",
    "X_train = pd.read_csv(\"data/processed/final/X_train.csv\")\n",
    "\n",
    "# Reapply feature engineering and encoding\n",
    "def process_new_properties(raw_df, training_features):\n",
    "    \"\"\"\n",
    "    Encodes and engineers features for new/unseen property data to align with model input schema.\n",
    "\n",
    "    Args:\n",
    "        raw_df (pd.DataFrame): Raw input dataframe of new houses\n",
    "        training_features (pd.DataFrame): Reference feature columns used during model training\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Fully aligned, processed feature set for prediction\n",
    "    \"\"\"\n",
    "    cat_cols = raw_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    df_encoded = pd.get_dummies(raw_df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    required_cols = ['YearBuilt', 'GrLivArea', 'LotArea', 'BsmtFinSF1', 'TotalBsmtSF', 'OverallQual', 'OverallCond']\n",
    "    if all(col in df_encoded.columns for col in required_cols):\n",
    "        df_encoded[\"Age\"] = 2025 - df_encoded[\"YearBuilt\"]\n",
    "        df_encoded[\"LivingLotRatio\"] = df_encoded[\"GrLivArea\"] / (df_encoded[\"LotArea\"] + 1)\n",
    "        df_encoded[\"FinishedBsmtRatio\"] = df_encoded[\"BsmtFinSF1\"] / (df_encoded[\"TotalBsmtSF\"] + 1)\n",
    "        df_encoded[\"OverallScore\"] = df_encoded[\"OverallQual\"] * df_encoded[\"OverallCond\"]\n",
    "        df_encoded.drop(columns=required_cols, inplace=True)\n",
    "\n",
    "    for col in training_features.columns:\n",
    "        if col not in df_encoded.columns:\n",
    "            df_encoded[col] = 0\n",
    "    df_encoded = df_encoded[training_features.columns]\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "inherited_processed = process_new_properties(inherited_raw_df.copy(), X_train)\n",
    "\n",
    "# Load the complete pipeline\n",
    "pipeline_path = \"outputs/models/final_random_forest_pipeline.pkl\"\n",
    "rf_pipeline = joblib.load(pipeline_path)\n",
    "print(\"[INFO] Loaded final_random_forest_pipeline.pkl\")\n",
    "\n",
    "# Run predictions\n",
    "preds_log = rf_pipeline.predict(inherited_processed)\n",
    "preds_sale = np.expm1(preds_log)\n",
    "\n",
    "# Combine with raw input for display\n",
    "inherited_raw_df[\"Predicted_LogSalePrice\"] = preds_log\n",
    "inherited_raw_df[\"Predicted_SalePrice\"] = preds_sale\n",
    "\n",
    "# Save to file\n",
    "output_path = \"outputs/predictions/inherited_price_forecast.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "inherited_raw_df.to_csv(output_path, index=False)\n",
    "print(f\"[SAVED] Forecast saved to: {output_path}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\n[INFO] Sample Predictions:\")\n",
    "display(inherited_raw_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e1305",
   "metadata": {},
   "source": [
    "## Deployment Documentation\n",
    "\n",
    "This section consolidates all steps taken to prepare the Heritage Housing machine learning pipeline for deployment. The notebook creates, validates, and serializes the entire prediction pipeline, which is now ready for integration into `main.py`.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "\n",
    "1. **Change Working Directory**  \n",
    "   Adjust the working directory so that relative paths correctly point to project root folders such as `/data`, `/outputs`, and `/utils`.\n",
    "\n",
    "2. **Import Dependencies**  \n",
    "   Load all necessary Python libraries required for preprocessing, modeling, serialization, and plotting.\n",
    "\n",
    "3. **Verify Dependencies**  \n",
    "   Check and display the versions of required libraries such as `pandas`, `scikit-learn`, `joblib`, etc. This helps ensure deployment compatibility.\n",
    "\n",
    "4. **Load Saved Artefacts**  \n",
    "   Import previously saved models, evaluation metrics, SHAP summaries, and train/test datasets stored under `/outputs/` and `/data/processed/final`.\n",
    "\n",
    "5. **Pipeline Design**  \n",
    "   Build reusable preprocessing logic that handles feature engineering, encoding, scaling, and column alignment.\n",
    "\n",
    "6. **Model Integration**  \n",
    "   Wrap the best-performing model (Random Forest) and the preprocessing pipeline using `Pipeline()`. This ensures consistency between training and deployment.\n",
    "\n",
    "7. **Serialization**  \n",
    "   Save the final pipeline as a `.pkl` file in `outputs/models/`. This is the primary artefact used during real-time inference.\n",
    "\n",
    "8. **Prediction on New Data**  \n",
    "   Preprocess and predict on new unseen data (e.g., `inherited_houses.csv`). Output is saved as a client-readable dashboard file.\n",
    "\n",
    "9. **Validation**  \n",
    "   Evaluate the model using actual test data and simulate validation on hypothetical inherited property targets. Output residuals and performance metrics are visualized and saved. Hypothetical evaluation was used due to lack of ground truth for inherited properties.\n",
    "\n",
    "10. **Output Verification**  \n",
    "    Confirm that all files were saved correctly and contain expected structure. Includes prediction results, residuals, and visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "### Deployment Setup\n",
    "\n",
    "#### Deployment Files:\n",
    "\n",
    "- `main.py`: Entry point for Streamlit or Flask app\n",
    "- `utils/deployment_pipeline.py`: Core logic for preprocessing + prediction\n",
    "- `requirements.txt`: Clean dependency list for Heroku\n",
    "- `Procfile`: Heroku process type definition (`web: streamlit run main.py`)\n",
    "- `.slugignore`: Excludes bulky outputs not needed during deployment\n",
    "\n",
    "#### Hosting Platform:\n",
    "You can now deploy using:\n",
    "- **Streamlit Cloud** for quick public dashboarding\n",
    "- **Heroku** using GitHub integration\n",
    "- **Render/Fly.io** if scaling is required\n",
    "\n",
    "---\n",
    "\n",
    "### Usage Notes\n",
    "\n",
    "- Execute this notebook sequentially from top to bottom to avoid file path issues.\n",
    "- The function `predict_from_raw()` in `deployment_pipeline.py` supports direct inference from new datasets.\n",
    "- All predictions and outputs are stored under `outputs/predictions/` and `data/processed/final/`.\n",
    "\n",
    "---\n",
    "\n",
    "### Integration Path\n",
    "\n",
    "1. Import `utils/deployment_pipeline.py` into `main.py`\n",
    "2. Load the pipeline with `load_pipeline(path)`\n",
    "3. Collect input from the user (form, file upload, etc.)\n",
    "4. Call `predict_from_raw()` to generate predictions\n",
    "5. Display or return results in Streamlit or via API\n",
    "\n",
    "---\n",
    "\n",
    "### Maintenance Guidelines\n",
    "\n",
    "| Area | Recommendation |\n",
    "|------|----------------|\n",
    "| **Model Refresh** | Retrain monthly or quarterly with updated sale records |\n",
    "| **Dependency Management** | Use consistent Python versions and lock requirements quarterly |\n",
    "| **Performance Monitoring** | Track model drift and prediction quality over time |\n",
    "| **Documentation** | Update README.md and notebook markdown as pipeline evolves |\n",
    "| **Scalability** | Move to a containerized or cloud-based solution if usage increases |\n",
    "\n",
    "---\n",
    "\n",
    "With this setup, Heritage Housing pipeline will be fully validated, serialized, and ready to deploy across environments â€” from local testing to cloud-hosted dashboards.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
